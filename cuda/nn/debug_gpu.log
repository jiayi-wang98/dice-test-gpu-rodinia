[Jiayi Test] __cudaRegisterFatBinary(
[Jiayi Test] g_debug_execution=0
[Jiayi Test] cudaRegisterFatBinaryInternal(


        *** GPGPU-Sim Simulator Version 4.0.0  [build gpgpu-sim_git-commit-0318ea6d3abb63e07870e8aa8636cda267c1ea6c_modified_32.0] ***


GPGPU-Sim PTX: simulation mode -1094795586 (can change with PTX_SIM_MODE_FUNC environment variable:
               1=functional simulation only, 0=detailed performance simulator)
GPGPU-Sim PTX: setting debug level to 3
GPGPU-Sim PTX: overriding embedded ptx with ptx file (PTX_SIM_USE_PTX_FILE is set)
DICE Metadata Parser: Debugging level = 3, 1
GPGPU-Sim: Configuration options:

-save_embedded_ptx                      1 # saves ptx files embedded in binary as <n>.ptx
-keep                                   1 # keep intermediate files created by GPGPU-Sim when interfacing with external programs
-gpgpu_ptx_save_converted_ptxplus                    1 # Saved converted ptxplus to a file
-gpgpu_occupancy_sm_number                   20 # The SM number to pass to ptxas when getting register usage for computing GPU occupancy. This parameter is required in the config.
-ptx_opcode_latency_int      4,13,4,5,145,32 # Opcode latencies for integers <ADD,MAX,MUL,MAD,DIV,SHFL>Default 1,1,19,25,145,32
-ptx_opcode_latency_fp          4,13,4,5,39 # Opcode latencies for single precision floating points <ADD,MAX,MUL,MAD,DIV>Default 1,1,1,1,30
-ptx_opcode_latency_dp         8,19,8,8,330 # Opcode latencies for double precision floating points <ADD,MAX,MUL,MAD,DIV>Default 8,8,8,8,335
-ptx_opcode_latency_sfu                    8 # Opcode latencies for SFU instructionsDefault 8
-ptx_opcode_latency_tesnor                   64 # Opcode latencies for Tensor instructionsDefault 64
-ptx_opcode_initiation_int          1,2,2,1,8,4 # Opcode initiation intervals for integers <ADD,MAX,MUL,MAD,DIV,SHFL>Default 1,1,4,4,32,4
-ptx_opcode_initiation_fp            1,2,1,1,4 # Opcode initiation intervals for single precision floating points <ADD,MAX,MUL,MAD,DIV>Default 1,1,1,1,5
-ptx_opcode_initiation_dp         8,16,8,8,130 # Opcode initiation intervals for double precision floating points <ADD,MAX,MUL,MAD,DIV>Default 8,8,8,8,130
-ptx_opcode_initiation_sfu                    8 # Opcode initiation intervals for sfu instructionsDefault 8
-ptx_opcode_initiation_tensor                   64 # Opcode initiation intervals for tensor instructionsDefault 64
-cdp_latency         7200,8000,100,12000,1600 # CDP API latency <cudaStreamCreateWithFlags, cudaGetParameterBufferV2_init_perWarp, cudaGetParameterBufferV2_perKernel, cudaLaunchDeviceV2_init_perWarp, cudaLaunchDevicV2_perKernel>Default 7200,8000,100,12000,1600
-network_mode                           1 # Interconnection network mode
-inter_config_file   config_fermi_islip.icnt # Interconnection network config file
-icnt_in_buffer_limit                   64 # in_buffer_limit
-icnt_out_buffer_limit                   64 # out_buffer_limit
-icnt_subnets                           2 # subnets
-icnt_arbiter_algo                      1 # arbiter_algo
-icnt_verbose                           0 # inct_verbose
-icnt_grant_cycles                      1 # grant_cycles
-gpgpu_ptx_use_cuobjdump                    1 # Use cuobjdump to extract ptx and sass from binaries
-gpgpu_experimental_lib_support                    0 # Try to extract code from cuda libraries [Broken because of unknown cudaGetExportTable]
-checkpoint_option                      0 #  checkpointing flag (0 = no checkpoint)
-checkpoint_kernel                      1 #  checkpointing during execution of which kernel (1- 1st kernel)
-checkpoint_CTA                         0 #  checkpointing after # of CTA (< less than total CTA)
-resume_option                          0 #  resume flag (0 = no resume)
-resume_kernel                          0 #  Resume from which kernel (1= 1st kernel)
-resume_CTA                             0 #  resume from which CTA 
-checkpoint_CTA_t                       0 #  resume from which CTA 
-checkpoint_insn_Y                      0 #  resume from which CTA 
-gpgpu_ptx_convert_to_ptxplus                    0 # Convert SASS (native ISA) to ptxplus and run ptxplus
-gpgpu_ptx_force_max_capability                   20 # Force maximum compute capability
-gpgpu_ptx_inst_debug_to_file                    0 # Dump executed instructions' debug information to file
-gpgpu_ptx_inst_debug_file       inst_debug.txt # Executed instructions' debug output file
-gpgpu_ptx_inst_debug_thread_uid                    1 # Thread UID for executed instructions' debug output
-gpgpu_simd_model                       1 # 1 = post-dominator
-gpgpu_shader_core_pipeline              1536:32 # shader core pipeline config, i.e., {<nthread>:<warpsize>}
-dice_cgra_core_max_threads                 1536 # DICE cgra core max number of threads config, i.e.,
-gpgpu_tex_cache:l1  N:4:128:24,L:R:m:N:L,T:128:4,128:2 # per-shader L1 texture cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>:<rf>}
-gpgpu_const_cache:l1 N:64:64:2,L:R:f:N:L,S:2:32,4 # per-shader L1 constant memory cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_cache:il1     N:4:128:4,L:R:f:N:L,S:2:32,4 # shader L1 instruction cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_cache:dl1     N:32:128:4,L:L:m:N:H,S:64:8,8 # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_l1_banks                         1 # The number of L1 cache banks
-gpgpu_l1_banks_byte_interleaving                   32 # l1 banks byte interleaving granularity
-gpgpu_l1_banks_hashing_function                    0 # l1 banks hashing function
-gpgpu_l1_latency                      35 # L1 Hit Latency
-gpgpu_smem_latency                    26 # smem Latency
-gpgpu_cache:dl1PrefL1                 none # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_cache:dl1PrefShared                 none # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_gmem_skip_L1D                    0 # global memory access skip L1D cache (implements -Xptxas -dlcm=cg, default=no skip)
-gpgpu_perfect_mem                      0 # enable perfect memory mode (no cache miss)
-n_regfile_gating_group                    4 # group of lanes that should be read/written together)
-gpgpu_clock_gated_reg_file                    0 # enable clock gated reg file for power calculations
-gpgpu_clock_gated_lanes                    0 # enable clock gated lanes for power calculations
-gpgpu_shader_registers                32768 # Number of registers per shader core. Limits number of concurrent CTAs. (default 8192)
-gpgpu_registers_per_block                 8192 # Maximum number of registers per CTA. (default 8192)
-gpgpu_ignore_resources_limitation                    0 # gpgpu_ignore_resources_limitation (default 0)
-gpgpu_shader_cta                       8 # Maximum number of concurrent CTAs in shader (default 8)
-gpgpu_num_cta_barriers                   16 # Maximum number of named barriers per CTA (default 16)
-gpgpu_n_clusters                      15 # number of processing clusters
-gpgpu_n_cores_per_cluster                    1 # number of simd cores per cluster
-gpgpu_n_cluster_ejection_buffer_size                   32 # number of packets in ejection buffer
-gpgpu_n_ldst_response_buffer_size                    2 # number of response packets in ld/st unit ejection buffer
-gpgpu_shmem_per_block                49152 # Size of shared memory per thread block or CTA (default 48kB)
-gpgpu_shmem_size                   49152 # Size of shared memory per shader core (default 16kB)
-gpgpu_adaptive_cache_config                    0 # adaptive_cache_config
-gpgpu_shmem_sizeDefault                49152 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_size_PrefL1                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_size_PrefShared                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_num_banks                   32 # Number of banks in the shared memory in each shader core (default 16)
-gpgpu_shmem_limited_broadcast                    0 # Limit shared memory to do one broadcast per cycle (default on)
-gpgpu_shmem_warp_parts                    1 # Number of portions a warp is divided into for shared memory bank conflict check 
-gpgpu_mem_unit_ports                    1 # The number of memory transactions allowed per core cycle
-gpgpu_shmem_warp_parts                    1 # Number of portions a warp is divided into for shared memory bank conflict check 
-gpgpu_warpdistro_shader                   -1 # Specify which shader core to collect the warp size distribution from
-gpgpu_warp_issue_shader                    0 # Specify which shader core to collect the warp issue distribution from
-gpgpu_local_mem_map                    1 # Mapping from local memory space address to simulated GPU physical address space (default = enabled)
-gpgpu_num_reg_banks                   16 # Number of register banks (default = 8)
-gpgpu_reg_bank_use_warp_id                    0 # Use warp ID in mapping registers to banks (default = off)
-gpgpu_sub_core_model                    0 # Sub Core Volta/Pascal model (default = off)
-gpgpu_enable_specialized_operand_collector                    1 # enable_specialized_operand_collector
-gpgpu_operand_collector_num_units_sp                    6 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_dp                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_units_sfu                    8 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_int                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_units_tensor_core                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_mem                    2 # number of collector units (default = 2)
-gpgpu_operand_collector_num_units_gen                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_in_ports_sp                    2 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_dp                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_in_ports_sfu                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_int                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_in_ports_tensor_core                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_mem                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_gen                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_sp                    2 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_dp                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_sfu                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_int                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_tensor_core                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_mem                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_gen                    0 # number of collector unit in ports (default = 0)
-gpgpu_coalesce_arch                   20 # Coalescing arch (GT200 = 13, Fermi = 20)
-gpgpu_num_sched_per_core                    2 # Number of warp schedulers per core
-gpgpu_max_insn_issue_per_warp                    1 # Max number of instructions that can be issued per warp in one cycle by scheduler (either 1 or 2)
-gpgpu_dual_issue_diff_exec_units                    1 # should dual issue use two different execution unit resources (Default = 1)
-gpgpu_simt_core_sim_order                    1 # Select the simulation order of cores in a cluster (0=Fix, 1=Round-Robin)
-gpgpu_pipeline_widths 2,0,0,1,1,2,0,0,1,1,2 # Pipeline widths ID_OC_SP,ID_OC_DP,ID_OC_INT,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_DP,OC_EX_INT,OC_EX_SFU,OC_EX_MEM,EX_WB,ID_OC_TENSOR_CORE,OC_EX_TENSOR_CORE
-gpgpu_tensor_core_avail                    0 # Tensor Core Available (default=0)
-gpgpu_num_sp_units                     2 # Number of SP units (default=1)
-gpgpu_num_dp_units                     0 # Number of DP units (default=0)
-gpgpu_num_int_units                    0 # Number of INT units (default=0)
-gpgpu_num_sfu_units                    1 # Number of SF units (default=1)
-gpgpu_num_tensor_core_units                    0 # Number of tensor_core units (default=1)
-gpgpu_num_mem_units                    1 # Number if ldst units (default=1) WARNING: not hooked up to anything
-gpgpu_scheduler                      gto # Scheduler configuration: < lrr | gto | two_level_active > If two_level_active:<num_active_warps>:<inner_prioritization>:<outer_prioritization>For complete list of prioritization values see shader.h enum scheduler_prioritization_typeDefault: gto
-gpgpu_concurrent_kernel_sm                    0 # Support concurrent kernels on a SM (default = disabled)
-gpgpu_perfect_inst_const_cache                    0 # perfect inst and const cache mode, so all inst and const hits in the cache(default = disabled)
-gpgpu_inst_fetch_throughput                    1 # the number of fetched intruction per warp each cycle
-gpgpu_reg_file_port_throughput                    1 # the number ports of the register file
-specialized_unit_1         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_2         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_3         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_4         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_5         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_6         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_7         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_8         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-gpgpu_perf_sim_memcpy                    1 # Fill the L2 cache on memcpy
-gpgpu_simple_dram_model                    0 # simple_dram_model with fixed latency and BW
-gpgpu_dram_scheduler                    1 # 0 = fifo, 1 = FR-FCFS (defaul)
-gpgpu_dram_partition_queues          64:64:64:64 # i2$:$2d:d2$:$2i
-l2_ideal                               0 # Use a ideal L2 cache that always hit
-gpgpu_cache:dl2     S:64:128:8,L:B:m:L:L,A:256:4,4:0,32 # unified banked L2 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>}
-gpgpu_cache:dl2_texture_only                    0 # L2 cache used for texture only
-gpgpu_n_mem                            6 # number of memory modules (e.g. memory controllers) in gpu
-gpgpu_n_sub_partition_per_mchannel                    2 # number of memory subpartition in each memory module
-gpgpu_n_mem_per_ctrlr                    2 # number of memory chips per memory controller
-gpgpu_memlatency_stat                   14 # track and display latency statistics 0x2 enables MC, 0x4 enables queue logs
-gpgpu_frfcfs_dram_sched_queue_size                   64 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_return_queue_size                  116 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_buswidth                    4 # default = 4 bytes (8 bytes per cycle at DDR)
-gpgpu_dram_burst_length                    8 # Burst length of each DRAM request (default = 4 data bus cycle)
-dram_data_command_freq_ratio                    4 # Frequency ratio between DRAM data bus and command bus (default = 2 times, i.e. DDR)
-gpgpu_dram_timing_opt nbk=16:CCD=2:RRD=6:RCD=12:RAS=28:RP=12:RC=40: CL=12:WL=4:CDLR=5:WR=12:nbkgrp=4:CCDL=3:RTPL=2 # DRAM timing parameters = {nbk:tCCD:tRRD:tRCD:tRAS:tRP:tRC:CL:WL:tCDLR:tWR:nbkgrp:tCCDL:tRTPL}
-gpgpu_l2_rop_latency                  120 # ROP queue latency (default 85)
-dram_latency                         100 # DRAM latency (default 30)
-dram_dual_bus_interface                    0 # dual_bus_interface (default = 0) 
-dram_bnk_indexing_policy                    0 # dram_bnk_indexing_policy (0 = normal indexing, 1 = Xoring with the higher bits) (Default = 0)
-dram_bnkgrp_indexing_policy                    1 # dram_bnkgrp_indexing_policy (0 = take higher bits, 1 = take lower bits) (Default = 0)
-dram_seperate_write_queue_enable                    0 # Seperate_Write_Queue_Enable
-dram_write_queue_size             32:28:16 # Write_Queue_Size
-dram_elimnate_rw_turnaround                    0 # elimnate_rw_turnaround i.e set tWTR and tRTW = 0
-icnt_flit_size                        40 # icnt_flit_size
-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.BBBCCCCB.CCSSSSSS # mapping memory address to dram model {dramid@<start bit>;<memory address map>}
-gpgpu_mem_addr_test                    0 # run sweep test to check address mapping for aliased address
-gpgpu_mem_address_mask                    1 # 0 = old addressing mask, 1 = new addressing mask, 2 = new add. mask + flipped bank sel and chip sel bits
-gpgpu_memory_partition_indexing                    0 # 0 = no indexing, 1 = bitwise xoring, 2 = IPoly, 3 = custom indexing
-gpuwattch_xml_file  gpuwattch_gtx480.xml # GPUWattch XML file
-power_simulation_enabled                    1 # Turn on power simulator (1=On, 0=Off)
-power_per_cycle_dump                    0 # Dump detailed power output each cycle
-power_trace_enabled                    0 # produce a file for the power trace (1=On, 0=Off)
-power_trace_zlevel                     6 # Compression level of the power trace output log (0=no comp, 9=highest)
-steady_power_levels_enabled                    0 # produce a file for the steady power levels (1=On, 0=Off)
-steady_state_definition                  8:4 # allowed deviation:number of samples
-gpgpu_max_cycle                        0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_insn                         0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_cta                          0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_completed_cta                    0 # terminates gpu simulation early (0 = no limit)
-gpgpu_runtime_stat                   500 # display runtime statistics such as dram utilization {<freq>:<flag>}
-liveness_message_freq                    1 # Minimum number of seconds between simulation liveness messages (0 = always print)
-gpgpu_compute_capability_major                    2 # Major compute capability version number
-gpgpu_compute_capability_minor                    0 # Minor compute capability version number
-gpgpu_flush_l1_cache                    1 # Flush L1 cache at the end of each kernel call
-gpgpu_flush_l2_cache                    0 # Flush L2 cache at the end of each kernel call
-gpgpu_deadlock_detect                    1 # Stop the simulation at deadlock (1=on (default), 0=off)
-gpgpu_ptx_instruction_classification                    0 # if enabled will classify ptx instruction types per kernel (Max 255 kernels now)
-gpgpu_ptx_sim_mode                     0 # Select between Performance (default) or Functional simulation (1)
-dice                                   0 # Choose if run on DICE mode
-gpgpu_clock_domains 700.0:700.0:700.0:924.0 # Clock Domain Frequencies in MhZ {<Core Clock>:<ICNT Clock>:<L2 Clock>:<DRAM Clock>}
-gpgpu_max_concurrent_kernel                    8 # maximum kernels that can run concurrently on GPU
-gpgpu_cflog_interval                    0 # Interval between each snapshot in control flow logger
-visualizer_enabled                     0 # Turn on visualizer output (1=On, 0=Off)
-visualizer_outputfile                 NULL # Specifies the output log file for visualizer
-visualizer_zlevel                      6 # Compression level of the visualizer output log (0=no comp, 9=highest)
-gpgpu_stack_size_limit                 1024 # GPU thread stack size
-gpgpu_heap_size_limit              8388608 # GPU malloc heap size 
-gpgpu_runtime_sync_depth_limit                    2 # GPU device runtime synchronize depth
-gpgpu_runtime_pending_launch_count_limit                 2048 # GPU device runtime pending launch count
-trace_enabled                          0 # Turn on traces
-trace_components                    none # comma seperated list of traces to enable. Complete list found in trace_streams.tup. Default none
-trace_sampling_core                    0 # The core which is printed using CORE_DPRINTF. Default 0
-trace_sampling_memory_partition                   -1 # The memory partition which is printed using MEMPART_DPRINTF. Default -1 (i.e. all)
-enable_ptx_file_line_stats                    1 # Turn on PTX source line statistic profiling. (1 = On)
-ptx_line_stats_filename gpgpu_inst_stats.txt # Output file for PTX source line statistics.
-gpgpu_kernel_launch_latency                    0 # Kernel launch latency in cycles. Default: 0
-gpgpu_cdp_enabled                      0 # Turn on CDP
-gpgpu_TB_launch_latency                    0 # thread block launch latency in cycles. Default: 0
DRAM Timing Options:
nbk                                    16 # number of banks
CCD                                     2 # column to column delay
RRD                                     6 # minimal delay between activation of rows in different banks
RCD                                    12 # row to column delay
RAS                                    28 # time needed to activate row
RP                                     12 # time needed to precharge (deactivate) row
RC                                     40 # row cycle time
CDLR                                    5 # switching from write to read (changes tWTR)
WR                                     12 # last data-in to row precharge
CL                                     12 # CAS latency
WL                                      4 # Write latency
nbkgrp                                  4 # number of bank groups
CCDL                                    3 # column to column delay between accesses to different bank groups
RTPL                                    2 # read to precharge delay between accesses to different bank groups
Total number of memory sub partition = 12
addr_dec_mask[CHIP]  = 0000000000000000 	high:64 low:0
addr_dec_mask[BK]    = 000000000000e100 	high:16 low:8
addr_dec_mask[ROW]   = 000000000fff0000 	high:28 low:16
addr_dec_mask[COL]   = 0000000000001eff 	high:13 low:0
addr_dec_mask[BURST] = 000000000000003f 	high:6 low:0
sub_partition_id_mask = 0000000000000100
GPGPU-Sim uArch: clock freqs: 700000000.000000:700000000.000000:700000000.000000:924000000.000000
GPGPU-Sim uArch: clock periods: 0.00000000142857142857:0.00000000142857142857:0.00000000142857142857:0.00000000108225108225
*** Initializing Memory Statistics ***
GPGPU-Sim uArch: interconnect node map (shaderID+MemID to icntID)
GPGPU-Sim uArch: Memory nodes ID start from index: 15
GPGPU-Sim uArch:    0   1   2   3   4
GPGPU-Sim uArch:    5   6   7   8   9
GPGPU-Sim uArch:   10  11  12  13  14
GPGPU-Sim uArch:   15  16  17  18  19
GPGPU-Sim uArch:   20  21  22  23  24
GPGPU-Sim uArch:   25  26
GPGPU-Sim uArch: interconnect node reverse map (icntID to shaderID+MemID)
GPGPU-Sim uArch: Memory nodes start from ID: 15
GPGPU-Sim uArch:    0   1   2   3   4
GPGPU-Sim uArch:    5   6   7   8   9
GPGPU-Sim uArch:   10  11  12  13  14
GPGPU-Sim uArch:   15  16  17  18  19
GPGPU-Sim uArch:   20  21  22  23  24
GPGPU-Sim uArch:   25  26
GPGPU-Sim uArch: performance model initialization complete.
[Jiayi Test]: start_sim_thread
GPGPU-Sim: *** simulation thread starting and spinning waiting for work ***
d7bb697bf3ae6b263c72c165e2a251e5  /data/jwang710/gpu-rodinia/cuda/nn/nn_cuda
Extracting PTX file and ptxas options    1: nn_cuda.1.sm_52.ptx -arch=sm_52
[Jiayi Test] get_app_binary(
self exe links to: /data/jwang710/gpu-rodinia/cuda/nn/nn_cuda
[Jiayi Test] get_app_binary(
self exe links to: /data/jwang710/gpu-rodinia/cuda/nn/nn_cuda
11.0
app_cuda_version: 11
GPGPU-Sim PTX: __cudaRegisterFatBinary, fat_cubin_handle = 1, filename=default
[Jiayi Test] get_app_binary(
self exe links to: /data/jwang710/gpu-rodinia/cuda/nn/nn_cuda
Running md5sum using "md5sum /data/jwang710/gpu-rodinia/cuda/nn/nn_cuda "
[Jiayi Test] get_app_binary(
self exe links to: /data/jwang710/gpu-rodinia/cuda/nn/nn_cuda
Extracting specific PTX file named nn_cuda.1.sm_52.ptx 
[Jiayi Test] CUDARTAPI __cudaRegisterFunction(
[Jiayi Test] cudaRegisterFunctionInternal(


GPGPU-Sim PTX: CUDA API function "void cudaRegisterFunctionInternal(void**, const char*, char*, const char*, int, uint3*, uint3*, dim3*, dim3*, gpgpu_context*)" has been called.
GPGPU-Sim PTX: __cudaRegisterFunction _Z6euclidP7latLongPfiff : hostFun 0x0x404490, fat_cubin_handle = 1
GPGPU-Sim PTX: Parsing nn_cuda.1.sm_52.ptx
[Jiayi Test] init_parser
 nn_cuda.1.sm_52.ptx:0 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:0 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:0 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:10 =>    (ptx_parser.cc:940) add_version_info
 nn_cuda.1.sm_52.ptx:15 =>    (ptx_parser.cc:149) start_function
 nn_cuda.1.sm_52.ptx:15 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:15 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:15 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:15 =>    (ptx_parser.cc:161) add_function_name _Z6euclidP7latLongPfiff (entrypoint)
 nn_cuda.1.sm_52.ptx:16 =>    (ptx_parser.cc:572) add_space_spec "param_space_unclassified"
 nn_cuda.1.sm_52.ptx:16 =>    (ptx_parser.cc:628) add_scalar_type_spec "U64_TYPE"
 nn_cuda.1.sm_52.ptx:16 =>    (ptx_parser.cc:295) set_variable_type space_spec=param_space_kernel scalar_type_spec=U64_TYPE
 nn_cuda.1.sm_52.ptx:16 =>    (ptx_parser.cc:342) add_identifier "_Z6euclidP7latLongPfiff_param_0" (0)
 nn_cuda.1.sm_52.ptx:16 =>    (ptx_parser.cc:539) add_function_arg "_Z6euclidP7latLongPfiff_param_0"
 nn_cuda.1.sm_52.ptx:16 =>    (ptx_parser.cc:192) add_directive
 nn_cuda.1.sm_52.ptx:16 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:17 =>    (ptx_parser.cc:572) add_space_spec "param_space_unclassified"
 nn_cuda.1.sm_52.ptx:17 =>    (ptx_parser.cc:628) add_scalar_type_spec "U64_TYPE"
 nn_cuda.1.sm_52.ptx:17 =>    (ptx_parser.cc:295) set_variable_type space_spec=param_space_kernel scalar_type_spec=U64_TYPE
 nn_cuda.1.sm_52.ptx:17 =>    (ptx_parser.cc:342) add_identifier "_Z6euclidP7latLongPfiff_param_1" (1)
 nn_cuda.1.sm_52.ptx:17 =>    (ptx_parser.cc:539) add_function_arg "_Z6euclidP7latLongPfiff_param_1"
 nn_cuda.1.sm_52.ptx:17 =>    (ptx_parser.cc:192) add_directive
 nn_cuda.1.sm_52.ptx:17 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:18 =>    (ptx_parser.cc:572) add_space_spec "param_space_unclassified"
 nn_cuda.1.sm_52.ptx:18 =>    (ptx_parser.cc:628) add_scalar_type_spec "U32_TYPE"
 nn_cuda.1.sm_52.ptx:18 =>    (ptx_parser.cc:295) set_variable_type space_spec=param_space_kernel scalar_type_spec=U32_TYPE
 nn_cuda.1.sm_52.ptx:18 =>    (ptx_parser.cc:342) add_identifier "_Z6euclidP7latLongPfiff_param_2" (2)
 nn_cuda.1.sm_52.ptx:18 =>    (ptx_parser.cc:539) add_function_arg "_Z6euclidP7latLongPfiff_param_2"
 nn_cuda.1.sm_52.ptx:18 =>    (ptx_parser.cc:192) add_directive
 nn_cuda.1.sm_52.ptx:18 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:19 =>    (ptx_parser.cc:572) add_space_spec "param_space_unclassified"
 nn_cuda.1.sm_52.ptx:19 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:19 =>    (ptx_parser.cc:295) set_variable_type space_spec=param_space_kernel scalar_type_spec=F32_TYPE
 nn_cuda.1.sm_52.ptx:19 =>    (ptx_parser.cc:342) add_identifier "_Z6euclidP7latLongPfiff_param_3" (3)
 nn_cuda.1.sm_52.ptx:19 =>    (ptx_parser.cc:539) add_function_arg "_Z6euclidP7latLongPfiff_param_3"
 nn_cuda.1.sm_52.ptx:19 =>    (ptx_parser.cc:192) add_directive
 nn_cuda.1.sm_52.ptx:19 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:20 =>    (ptx_parser.cc:572) add_space_spec "param_space_unclassified"
 nn_cuda.1.sm_52.ptx:20 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:20 =>    (ptx_parser.cc:295) set_variable_type space_spec=param_space_kernel scalar_type_spec=F32_TYPE
 nn_cuda.1.sm_52.ptx:21 =>    (ptx_parser.cc:342) add_identifier "_Z6euclidP7latLongPfiff_param_4" (4)
 nn_cuda.1.sm_52.ptx:21 =>    (ptx_parser.cc:539) add_function_arg "_Z6euclidP7latLongPfiff_param_4"
 nn_cuda.1.sm_52.ptx:21 =>    (ptx_parser.cc:192) add_directive
 nn_cuda.1.sm_52.ptx:21 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:23 =>    (ptx_parser.cc:572) add_space_spec "reg_space"
 nn_cuda.1.sm_52.ptx:23 =>    (ptx_parser.cc:628) add_scalar_type_spec "PRED_TYPE"
 nn_cuda.1.sm_52.ptx:23 =>    (ptx_parser.cc:295) set_variable_type space_spec=reg_space scalar_type_spec=PRED_TYPE
 nn_cuda.1.sm_52.ptx:23 =>    (ptx_parser.cc:342) add_identifier "%p0" (5)
 nn_cuda.1.sm_52.ptx:23 =>    (ptx_parser.cc:342) add_identifier "%p1" (6)
 nn_cuda.1.sm_52.ptx:23 =>    (ptx_parser.cc:284) add_variables
 nn_cuda.1.sm_52.ptx:23 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:23 =>    (ptx_parser.cc:192) add_directive
 nn_cuda.1.sm_52.ptx:23 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:572) add_space_spec "reg_space"
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:295) set_variable_type space_spec=reg_space scalar_type_spec=F32_TYPE
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:342) add_identifier "%f0" (7)
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:342) add_identifier "%f1" (8)
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:342) add_identifier "%f2" (9)
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:342) add_identifier "%f3" (10)
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:342) add_identifier "%f4" (11)
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:342) add_identifier "%f5" (12)
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:342) add_identifier "%f6" (13)
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:342) add_identifier "%f7" (14)
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:342) add_identifier "%f8" (15)
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:342) add_identifier "%f9" (16)
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:284) add_variables
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:192) add_directive
 nn_cuda.1.sm_52.ptx:24 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:572) add_space_spec "reg_space"
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:628) add_scalar_type_spec "B32_TYPE"
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:295) set_variable_type space_spec=reg_space scalar_type_spec=B32_TYPE
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:342) add_identifier "%r0" (17)
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:342) add_identifier "%r1" (18)
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:342) add_identifier "%r2" (19)
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:342) add_identifier "%r3" (20)
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:342) add_identifier "%r4" (21)
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:342) add_identifier "%r5" (22)
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:342) add_identifier "%r6" (23)
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:342) add_identifier "%r7" (24)
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:342) add_identifier "%r8" (25)
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:284) add_variables
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:192) add_directive
 nn_cuda.1.sm_52.ptx:25 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:572) add_space_spec "reg_space"
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:628) add_scalar_type_spec "B64_TYPE"
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:295) set_variable_type space_spec=reg_space scalar_type_spec=B64_TYPE
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:342) add_identifier "%rd0" (26)
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:342) add_identifier "%rd1" (27)
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:342) add_identifier "%rd2" (28)
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:342) add_identifier "%rd3" (29)
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:342) add_identifier "%rd4" (30)
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:342) add_identifier "%rd5" (31)
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:342) add_identifier "%rd6" (32)
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:342) add_identifier "%rd7" (33)
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:342) add_identifier "%rd8" (34)
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:284) add_variables
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:192) add_directive
 nn_cuda.1.sm_52.ptx:26 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:29 =>    (ptx_parser.cc:572) add_space_spec "param_space_unclassified"
 nn_cuda.1.sm_52.ptx:29 =>    (ptx_parser.cc:628) add_scalar_type_spec "U64_TYPE"
 nn_cuda.1.sm_52.ptx:29 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:29 =>    (ptx_parser.cc:920) add_address_operand
 nn_cuda.1.sm_52.ptx:29 =>    (ptx_parser.cc:767) add_memory_operand
 nn_cuda.1.sm_52.ptx:29 =>    (ptx_parser.cc:271) add_instruction: ld
 nn_cuda.1.sm_52.ptx:29 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:29 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:30 =>    (ptx_parser.cc:572) add_space_spec "param_space_unclassified"
 nn_cuda.1.sm_52.ptx:30 =>    (ptx_parser.cc:628) add_scalar_type_spec "U64_TYPE"
 nn_cuda.1.sm_52.ptx:30 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:30 =>    (ptx_parser.cc:920) add_address_operand
 nn_cuda.1.sm_52.ptx:30 =>    (ptx_parser.cc:767) add_memory_operand
 nn_cuda.1.sm_52.ptx:30 =>    (ptx_parser.cc:271) add_instruction: ld
 nn_cuda.1.sm_52.ptx:30 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:30 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:31 =>    (ptx_parser.cc:572) add_space_spec "param_space_unclassified"
 nn_cuda.1.sm_52.ptx:31 =>    (ptx_parser.cc:628) add_scalar_type_spec "U32_TYPE"
 nn_cuda.1.sm_52.ptx:31 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:31 =>    (ptx_parser.cc:920) add_address_operand
 nn_cuda.1.sm_52.ptx:31 =>    (ptx_parser.cc:767) add_memory_operand
 nn_cuda.1.sm_52.ptx:31 =>    (ptx_parser.cc:271) add_instruction: ld
 nn_cuda.1.sm_52.ptx:31 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:31 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:32 =>    (ptx_parser.cc:572) add_space_spec "param_space_unclassified"
 nn_cuda.1.sm_52.ptx:32 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:32 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:32 =>    (ptx_parser.cc:920) add_address_operand
 nn_cuda.1.sm_52.ptx:32 =>    (ptx_parser.cc:767) add_memory_operand
 nn_cuda.1.sm_52.ptx:32 =>    (ptx_parser.cc:271) add_instruction: ld
 nn_cuda.1.sm_52.ptx:32 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:32 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:33 =>    (ptx_parser.cc:572) add_space_spec "param_space_unclassified"
 nn_cuda.1.sm_52.ptx:33 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:33 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:33 =>    (ptx_parser.cc:920) add_address_operand
 nn_cuda.1.sm_52.ptx:33 =>    (ptx_parser.cc:767) add_memory_operand
 nn_cuda.1.sm_52.ptx:33 =>    (ptx_parser.cc:271) add_instruction: ld
 nn_cuda.1.sm_52.ptx:33 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:33 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:34 =>    (ptx_parser.cc:628) add_scalar_type_spec "U32_TYPE"
 nn_cuda.1.sm_52.ptx:34 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:34 =>    (ptx_parser.cc:762) add_builtin_operand
 nn_cuda.1.sm_52.ptx:34 =>    (ptx_parser.cc:271) add_instruction: mov
 nn_cuda.1.sm_52.ptx:34 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:34 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:35 =>    (ptx_parser.cc:628) add_scalar_type_spec "U32_TYPE"
 nn_cuda.1.sm_52.ptx:35 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:35 =>    (ptx_parser.cc:762) add_builtin_operand
 nn_cuda.1.sm_52.ptx:35 =>    (ptx_parser.cc:271) add_instruction: mov
 nn_cuda.1.sm_52.ptx:35 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:35 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:36 =>    (ptx_parser.cc:628) add_scalar_type_spec "U32_TYPE"
 nn_cuda.1.sm_52.ptx:36 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:36 =>    (ptx_parser.cc:762) add_builtin_operand
 nn_cuda.1.sm_52.ptx:36 =>    (ptx_parser.cc:271) add_instruction: mov
 nn_cuda.1.sm_52.ptx:36 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:36 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:37 =>    (ptx_parser.cc:670) add_option
 nn_cuda.1.sm_52.ptx:37 =>    (ptx_parser.cc:628) add_scalar_type_spec "S32_TYPE"
 nn_cuda.1.sm_52.ptx:37 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:37 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:37 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:37 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:37 =>    (ptx_parser.cc:271) add_instruction: mad
 nn_cuda.1.sm_52.ptx:37 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:37 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:38 =>    (ptx_parser.cc:628) add_scalar_type_spec "U32_TYPE"
 nn_cuda.1.sm_52.ptx:38 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:38 =>    (ptx_parser.cc:762) add_builtin_operand
 nn_cuda.1.sm_52.ptx:38 =>    (ptx_parser.cc:271) add_instruction: mov
 nn_cuda.1.sm_52.ptx:38 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:38 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:39 =>    (ptx_parser.cc:628) add_scalar_type_spec "U32_TYPE"
 nn_cuda.1.sm_52.ptx:39 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:39 =>    (ptx_parser.cc:762) add_builtin_operand
 nn_cuda.1.sm_52.ptx:39 =>    (ptx_parser.cc:271) add_instruction: mov
 nn_cuda.1.sm_52.ptx:39 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:39 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:40 =>    (ptx_parser.cc:670) add_option
 nn_cuda.1.sm_52.ptx:40 =>    (ptx_parser.cc:628) add_scalar_type_spec "S32_TYPE"
 nn_cuda.1.sm_52.ptx:40 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:40 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:40 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:40 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:40 =>    (ptx_parser.cc:271) add_instruction: mad
 nn_cuda.1.sm_52.ptx:40 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:40 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:41 =>    (ptx_parser.cc:670) add_option
 nn_cuda.1.sm_52.ptx:41 =>    (ptx_parser.cc:628) add_scalar_type_spec "S32_TYPE"
 nn_cuda.1.sm_52.ptx:41 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:41 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:41 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:41 =>    (ptx_parser.cc:271) add_instruction: setp
 nn_cuda.1.sm_52.ptx:41 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:41 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:42 =>    (ptx_parser.cc:657) add_pred
 nn_cuda.1.sm_52.ptx:42 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:42 =>    (ptx_parser.cc:271) add_instruction: bra
 nn_cuda.1.sm_52.ptx:42 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:42 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:44 =>    (ptx_parser.cc:670) add_option
 nn_cuda.1.sm_52.ptx:44 =>    (ptx_parser.cc:572) add_space_spec "global_space"
 nn_cuda.1.sm_52.ptx:44 =>    (ptx_parser.cc:628) add_scalar_type_spec "U64_TYPE"
 nn_cuda.1.sm_52.ptx:44 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:44 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:44 =>    (ptx_parser.cc:271) add_instruction: cvta
 nn_cuda.1.sm_52.ptx:44 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:44 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:45 =>    (ptx_parser.cc:670) add_option
 nn_cuda.1.sm_52.ptx:45 =>    (ptx_parser.cc:628) add_scalar_type_spec "S32_TYPE"
 nn_cuda.1.sm_52.ptx:45 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:45 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:45 =>    (ptx_parser.cc:876) add_literal_int
 nn_cuda.1.sm_52.ptx:45 =>    (ptx_parser.cc:271) add_instruction: mul
 nn_cuda.1.sm_52.ptx:45 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:45 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:46 =>    (ptx_parser.cc:628) add_scalar_type_spec "S64_TYPE"
 nn_cuda.1.sm_52.ptx:46 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:46 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:46 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:46 =>    (ptx_parser.cc:271) add_instruction: add
 nn_cuda.1.sm_52.ptx:46 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:46 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:47 =>    (ptx_parser.cc:670) add_option
 nn_cuda.1.sm_52.ptx:47 =>    (ptx_parser.cc:572) add_space_spec "global_space"
 nn_cuda.1.sm_52.ptx:47 =>    (ptx_parser.cc:628) add_scalar_type_spec "U64_TYPE"
 nn_cuda.1.sm_52.ptx:47 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:47 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:47 =>    (ptx_parser.cc:271) add_instruction: cvta
 nn_cuda.1.sm_52.ptx:47 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:47 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:48 =>    (ptx_parser.cc:670) add_option
 nn_cuda.1.sm_52.ptx:48 =>    (ptx_parser.cc:628) add_scalar_type_spec "S32_TYPE"
 nn_cuda.1.sm_52.ptx:48 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:48 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:48 =>    (ptx_parser.cc:876) add_literal_int
 nn_cuda.1.sm_52.ptx:48 =>    (ptx_parser.cc:271) add_instruction: mul
 nn_cuda.1.sm_52.ptx:48 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:48 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:49 =>    (ptx_parser.cc:628) add_scalar_type_spec "S64_TYPE"
 nn_cuda.1.sm_52.ptx:49 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:49 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:49 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:49 =>    (ptx_parser.cc:271) add_instruction: add
 nn_cuda.1.sm_52.ptx:49 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:49 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:50 =>    (ptx_parser.cc:572) add_space_spec "global_space"
 nn_cuda.1.sm_52.ptx:50 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:50 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:50 =>    (ptx_parser.cc:920) add_address_operand
 nn_cuda.1.sm_52.ptx:50 =>    (ptx_parser.cc:767) add_memory_operand
 nn_cuda.1.sm_52.ptx:50 =>    (ptx_parser.cc:271) add_instruction: ld
 nn_cuda.1.sm_52.ptx:50 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:50 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:51 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:51 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:51 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:51 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:51 =>    (ptx_parser.cc:271) add_instruction: sub
 nn_cuda.1.sm_52.ptx:51 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:51 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:52 =>    (ptx_parser.cc:572) add_space_spec "global_space"
 nn_cuda.1.sm_52.ptx:52 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:52 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:52 =>    (ptx_parser.cc:920) add_address_operand
 nn_cuda.1.sm_52.ptx:52 =>    (ptx_parser.cc:767) add_memory_operand
 nn_cuda.1.sm_52.ptx:52 =>    (ptx_parser.cc:271) add_instruction: ld
 nn_cuda.1.sm_52.ptx:52 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:52 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:53 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:53 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:53 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:53 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:53 =>    (ptx_parser.cc:271) add_instruction: sub
 nn_cuda.1.sm_52.ptx:53 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:53 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:54 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:54 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:54 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:54 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:54 =>    (ptx_parser.cc:271) add_instruction: mul
 nn_cuda.1.sm_52.ptx:54 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:54 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:55 =>    (ptx_parser.cc:670) add_option
 nn_cuda.1.sm_52.ptx:55 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:55 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:55 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:55 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:55 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:55 =>    (ptx_parser.cc:271) add_instruction: fma
 nn_cuda.1.sm_52.ptx:55 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:55 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:56 =>    (ptx_parser.cc:670) add_option
 nn_cuda.1.sm_52.ptx:56 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:56 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:56 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:56 =>    (ptx_parser.cc:271) add_instruction: sqrt
 nn_cuda.1.sm_52.ptx:56 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:56 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:57 =>    (ptx_parser.cc:572) add_space_spec "global_space"
 nn_cuda.1.sm_52.ptx:57 =>    (ptx_parser.cc:628) add_scalar_type_spec "F32_TYPE"
 nn_cuda.1.sm_52.ptx:57 =>    (ptx_parser.cc:920) add_address_operand
 nn_cuda.1.sm_52.ptx:57 =>    (ptx_parser.cc:767) add_memory_operand
 nn_cuda.1.sm_52.ptx:57 =>    (ptx_parser.cc:891) add_scalar_operand
 nn_cuda.1.sm_52.ptx:57 =>    (ptx_parser.cc:271) add_instruction: st
 nn_cuda.1.sm_52.ptx:57 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:57 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:60 =>    (ptx_parser.cc:643) add_label
 nn_cuda.1.sm_52.ptx:60 =>    (ptx_parser.cc:271) add_instruction: $L__BB0_2
 nn_cuda.1.sm_52.ptx:60 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:60 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:60 =>    (ptx_parser.cc:271) add_instruction: ret
 nn_cuda.1.sm_52.ptx:60 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:60 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:62 =>    (ptx_parser.cc:199) end_function
 nn_cuda.1.sm_52.ptx:62 =>    (ptx_parser.cc:75) init_directive_state
 nn_cuda.1.sm_52.ptx:62 =>    (ptx_parser.cc:90) init_instruction_state
 nn_cuda.1.sm_52.ptx:62 =>    (ptx_parser.cc:75) init_directive_state
GPGPU-Sim PTX: instruction assembly for function '_Z6euclidP7latLongPfiff'...   done.
 nn_cuda.1.sm_52.ptx:62 =>    (ptx_parser.cc:216) function _Z6euclidP7latLongPfiff, PC = 0

GPGPU-Sim PTX: finished parsing EMBEDDED .ptx file nn_cuda.1.sm_52.ptx
[Jiayi Test] dump symbol table


Symbol table for "global_allfiles":
       _Z6euclidP7latLongPfiff : uid:2, decl:nn_cuda.1.sm_52.ptx:15, type:0x604000039b10,  is_func_addr  0x6160011c4f80 

[Jiayi Test] finished dumping symbol table


GPGPU-Sim PTX: CUDA API function "int cuda_runtime_api::load_static_globals(symbol_table*, unsigned int, unsigned int, gpgpu_t*)" has been called.
GPGPU-Sim PTX: loading globals with explicit initializers... 
GPGPU-Sim PTX: finished loading globals (0 bytes total).


GPGPU-Sim PTX: CUDA API function "int cuda_runtime_api::load_constants(symbol_table*, addr_t, gpgpu_t*)" has been called.
GPGPU-Sim PTX: loading constants with explicit initializers...  done.
GPGPU-Sim PTX: Loading PTXInfo from nn_cuda.1.sm_52.ptx
GPGPU-Sim PTX: Kernel '_Z6euclidP7latLongPfiff' : regs=10, lmem=0, smem=0, cmem=356
[Jiayi Test] __cudaRegisterFatBinaryEnd


GPGPU-Sim PTX: CUDA API function "void __cudaRegisterFatBinaryEnd(void**)" has been called.
[Jiayi Test] cudaGetDevicePropertiesInternal


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaGetDevicePropertiesInternal(cudaDeviceProp*, int, gpgpu_context*)" has been called.
[Jiayi Test]: start_sim_thread


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaThreadSynchronizeInternal(gpgpu_context*)" has been called.
GPGPU-Sim: synchronize waiting for inactive GPU simulation
GPGPU-Sim API: Stream Manager State
GPGPU-Sim: detected inactive GPU simulation thread


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaMemGetInfo(size_t*, size_t*)" has been called.


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaThreadSynchronizeInternal(gpgpu_context*)" has been called.
GPGPU-Sim: synchronize waiting for inactive GPU simulation
GPGPU-Sim API: Stream Manager State
GPGPU-Sim: detected inactive GPU simulation thread


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaMallocInternal(void**, size_t, gpgpu_context*)" has been called.
GPGPU-Sim PTX: allocating 342112 bytes on GPU starting at address 0xc0000000
GPGPU-Sim PTX: cudaMallocing 342112 bytes starting at 0xc0000000..


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaMallocInternal(void**, size_t, gpgpu_context*)" has been called.
GPGPU-Sim PTX: allocating 171056 bytes on GPU starting at address 0xc0053900
GPGPU-Sim PTX: cudaMallocing 171056 bytes starting at 0xc0053900..


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaMemcpyInternal(void*, const void*, size_t, cudaMemcpyKind, gpgpu_context*)" has been called.
GPGPU-Sim PTX: cudaMemcpy(): devPtr = 0xc0000000
GPGPU-Sim API: Stream Manager State
Jiayi Test: CUstream_st::print
GPGPU-Sim API:    stream 0 has 1 operations
GPGPU-Sim API:       0 :  stream operation memcpy host-to-device
GPGPU-Sim: ** START simulation thread (detected work) **
GPGPU-Sim API: Stream Manager State
Jiayi Test: CUstream_st::print
GPGPU-Sim API:    stream 0 has 1 operations
GPGPU-Sim API:       0 :  stream operation memcpy host-to-device
GPGPU-Sim API: stream 0 performing memcpy host-to-device
GPGPU-Sim PTX: copying 342112 bytes from CPU[0x7fdd3f45a800] to GPU[0xc0000000] ...  copying...
 done.
GPGPU-Sim: ** STOP simulation thread (no work) **
GPGPU-Sim: *** simulation thread starting and spinning waiting for work ***


GPGPU-Sim PTX: CUDA API function "unsigned int __cudaPushCallConfiguration(dim3, dim3, size_t, CUstream_st*)" has been called.
[Jiayi Test] cudaConfigureCallInternal(


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaConfigureCallInternal(dim3, dim3, size_t, cudaStream_t, gpgpu_context*)" has been called.


GPGPU-Sim PTX: CUDA API function "cudaError_t __cudaPopCallConfiguration(dim3*, dim3*, size_t*, void*)" has been called.
[Jiayi Test] cudaLaunchKernel(2762)
[Jiayi Test] cuLaunchKernelInternal1957()


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaLaunchKernelInternal(const char*, dim3, dim3, const void**, size_t, cudaStream_t, gpgpu_context*)" has been called.


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaSetupArgumentInternal(const void*, size_t, size_t, gpgpu_context*)" has been called.
GPGPU-Sim PTX: Setting up arguments for 8 bytes starting at 0x7ffd6ae09870..


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaSetupArgumentInternal(const void*, size_t, size_t, gpgpu_context*)" has been called.
GPGPU-Sim PTX: Setting up arguments for 8 bytes starting at 0x7ffd6ae098b0..


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaSetupArgumentInternal(const void*, size_t, size_t, gpgpu_context*)" has been called.
GPGPU-Sim PTX: Setting up arguments for 4 bytes starting at 0x7ffd6ae09730..


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaSetupArgumentInternal(const void*, size_t, size_t, gpgpu_context*)" has been called.
GPGPU-Sim PTX: Setting up arguments for 4 bytes starting at 0x7ffd6ae09770..


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaSetupArgumentInternal(const void*, size_t, size_t, gpgpu_context*)" has been called.
GPGPU-Sim PTX: Setting up arguments for 4 bytes starting at 0x7ffd6ae097b0..
[Jiayi Test] cudaLaunchInternal


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaLaunchInternal(const char*, gpgpu_context*)" has been called.

GPGPU-Sim PTX: cudaLaunch for 0x0x404490 (mode=performance simulation) on stream 0


GPGPU-Sim PTX: CUDA API function "kernel_info_t* cuda_runtime_api::gpgpu_cuda_ptx_sim_init_grid(const char*, gpgpu_ptx_sim_arg_list_t, dim3, dim3, CUctx_st*)" has been called.
GPGPU-Sim PTX: finding reconvergence points for '_Z6euclidP7latLongPfiff'...
GPGPU-Sim PTX: Finding dominators for '_Z6euclidP7latLongPfiff'...
GPGPU-Sim PTX: Finding immediate dominators for '_Z6euclidP7latLongPfiff'...
Printing basic blocks for function '_Z6euclidP7latLongPfiff':
bb_00	:  PC=0x000 (nn_cuda.1.sm_52.ptx:29) ld.param.u64 %rd1, [_Z6euclidP7latLongPfiff_param_0];
bb_00	:  PC=0x008 (nn_cuda.1.sm_52.ptx:30) ld.param.u64 %rd2, [_Z6euclidP7latLongPfiff_param_1];
bb_00	:  PC=0x010 (nn_cuda.1.sm_52.ptx:31) ld.param.u32 %r2, [_Z6euclidP7latLongPfiff_param_2];
bb_00	:  PC=0x018 (nn_cuda.1.sm_52.ptx:32) ld.param.f32 %f1, [_Z6euclidP7latLongPfiff_param_3];
bb_00	:  PC=0x020 (nn_cuda.1.sm_52.ptx:33) ld.param.f32 %f2, [_Z6euclidP7latLongPfiff_param_4];
bb_00	:  PC=0x028 (nn_cuda.1.sm_52.ptx:34) mov.u32 %r3, %ctaid.y;
bb_00	:  PC=0x030 (nn_cuda.1.sm_52.ptx:35) mov.u32 %r4, %nctaid.x;
bb_00	:  PC=0x038 (nn_cuda.1.sm_52.ptx:36) mov.u32 %r5, %ctaid.x;
bb_00	:  PC=0x040 (nn_cuda.1.sm_52.ptx:37) mad.lo.s32 %r6, %r4, %r3, %r5;
bb_00	:  PC=0x048 (nn_cuda.1.sm_52.ptx:38) mov.u32 %r7, %ntid.x;
bb_00	:  PC=0x050 (nn_cuda.1.sm_52.ptx:39) mov.u32 %r8, %tid.x;
bb_00	:  PC=0x058 (nn_cuda.1.sm_52.ptx:40) mad.lo.s32 %r1, %r6, %r7, %r8;
bb_00	:  PC=0x060 (nn_cuda.1.sm_52.ptx:41) setp.ge.s32 %p1, %r1, %r2;
bb_00	:  PC=0x068 (nn_cuda.1.sm_52.ptx:42) @%p1 bra $L__BB0_2;

bb_01	:  PC=0x070 (nn_cuda.1.sm_52.ptx:44) cvta.to.global.u64 %rd3, %rd2;
bb_01	:  PC=0x078 (nn_cuda.1.sm_52.ptx:45) mul.wide.s32 %rd4, %r1, 4;
bb_01	:  PC=0x080 (nn_cuda.1.sm_52.ptx:46) add.s64 %rd5, %rd3, %rd4;
bb_01	:  PC=0x088 (nn_cuda.1.sm_52.ptx:47) cvta.to.global.u64 %rd6, %rd1;
bb_01	:  PC=0x090 (nn_cuda.1.sm_52.ptx:48) mul.wide.s32 %rd7, %r1, 8;
bb_01	:  PC=0x098 (nn_cuda.1.sm_52.ptx:49) add.s64 %rd8, %rd6, %rd7;
bb_01	:  PC=0x0a0 (nn_cuda.1.sm_52.ptx:50) ld.global.f32 %f3, [%rd8];
bb_01	:  PC=0x0a8 (nn_cuda.1.sm_52.ptx:51) sub.f32 %f4, %f1, %f3;
bb_01	:  PC=0x0b0 (nn_cuda.1.sm_52.ptx:52) ld.global.f32 %f5, [%rd8+4];
bb_01	:  PC=0x0b8 (nn_cuda.1.sm_52.ptx:53) sub.f32 %f6, %f2, %f5;
bb_01	:  PC=0x0c0 (nn_cuda.1.sm_52.ptx:54) mul.f32 %f7, %f6, %f6;
bb_01	:  PC=0x0c8 (nn_cuda.1.sm_52.ptx:55) fma.rn.f32 %f8, %f4, %f4, %f7;
bb_01	:  PC=0x0d0 (nn_cuda.1.sm_52.ptx:56) sqrt.rn.f32 %f9, %f8;
bb_01	:  PC=0x0d8 (nn_cuda.1.sm_52.ptx:57) st.global.f32 [%rd5], %f9;

bb_02	:                 (nn_cuda.1.sm_52.ptx:60) ret;
bb_02	:  PC=0x0e0 (nn_cuda.1.sm_52.ptx:60) ret;

Summary of basic blocks for '_Z6euclidP7latLongPfiff':
bb_00	: first: ld	 last: bra	
bb_01	: first: cvta	 last: st	
bb_02	: first: ret	 last: ret	
bb_03	: first: NULL	 last: NULL	

Printing basic blocks links for function '_Z6euclidP7latLongPfiff':
ID: 0	:Successors: 1 2
ID: 1	:Predecessors: 0	Successors: 2
ID: 2	:Predecessors: 0 1	Successors: 3
ID: 3	:Predecessors: 2	
Basic Block in DOT
digraph _Z6euclidP7latLongPfiff {
	0 -> 1; 0 -> 2; 
	1 -> 2; 
	2 -> 3; 
	
}
Printing dominators for function '_Z6euclidP7latLongPfiff':
ID: 0	: 0
ID: 1	: 0 1
ID: 2	: 0 2
ID: 3	: 0 2 3
GPGPU-Sim PTX: Finding postdominators for '_Z6euclidP7latLongPfiff'...
GPGPU-Sim PTX: Finding immediate postdominators for '_Z6euclidP7latLongPfiff'...
Printing postdominators for function '_Z6euclidP7latLongPfiff':
ID: 0	: 0 2 3
ID: 1	: 1 2 3
ID: 2	: 2 3
ID: 3	: 3
Printing immediate postdominators for function '_Z6euclidP7latLongPfiff':
ID: 0	:2
ID: 1	:2
ID: 2	:3
ID: 3	:-1
GPGPU-Sim PTX: pre-decoding instructions for '_Z6euclidP7latLongPfiff'...
GPGPU-Sim PTX: reconvergence points for _Z6euclidP7latLongPfiff...
GPGPU-Sim PTX:  1 (potential) branch divergence @  PC=0x068 (nn_cuda.1.sm_52.ptx:42) @%p1 bra $L__BB0_2;
GPGPU-Sim PTX:    immediate post dominator      @  PC=0x0e0 (nn_cuda.1.sm_52.ptx:60) ret;
GPGPU-Sim PTX: ... end of reconvergence points for _Z6euclidP7latLongPfiff
GPGPU-Sim PTX: ... done pre-decoding instructions for '_Z6euclidP7latLongPfiff'.
GPGPU-Sim PTX: pushing kernel '_Z6euclidP7latLongPfiff' to stream 0, gridDim= (168,1,1) blockDim = (256,1,1) 
GPGPU-Sim API: Stream Manager State
Jiayi Test: CUstream_st::print
GPGPU-Sim API:    stream 0 has 1 operations
GPGPU-Sim API:       0 :  stream operation kernel
GPGPU-Sim: ** START simulation thread (detected work) **
GPGPU-Sim API: Stream Manager State
Jiayi Test: CUstream_st::print
GPGPU-Sim API:    stream 0 has 1 operations
GPGPU-Sim API:       0 :  stream operation kernel
GPGPU-Sim API: stream 0 performing kernel 1: '_Z6euclidP7latLongPfiff' transfer to GPU hardware scheduler
GPGPU-Sim uArch: Shader 0 bind to kernel 1 '_Z6euclidP7latLongPfiff'
GPGPU-Sim uArch: CTA/core = 6, limited by: threads
  <CTA alloc> : sm_idx=0 sid=0 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 1 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=1 sid=1 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 2 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=2 sid=2 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 3 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=3 sid=3 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 4 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=4 sid=4 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 5 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=5 sid=5 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 6 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=6 sid=6 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 7 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=7 sid=7 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 8 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=8 sid=8 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 9 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=9 sid=9 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 10 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=10 sid=10 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 11 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=11 sid=11 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 12 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=12 sid=12 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 13 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=13 sid=13 max_cta_per_sm=6
GPGPU-Sim uArch: Shader 14 bind to kernel 1 '_Z6euclidP7latLongPfiff'
  <CTA alloc> : sm_idx=14 sid=14 max_cta_per_sm=6
  <CTA alloc> : sm_idx=15 sid=0 max_cta_per_sm=6
  <CTA alloc> : sm_idx=16 sid=1 max_cta_per_sm=6
  <CTA alloc> : sm_idx=17 sid=2 max_cta_per_sm=6
  <CTA alloc> : sm_idx=18 sid=3 max_cta_per_sm=6
  <CTA alloc> : sm_idx=19 sid=4 max_cta_per_sm=6
  <CTA alloc> : sm_idx=20 sid=5 max_cta_per_sm=6
  <CTA alloc> : sm_idx=21 sid=6 max_cta_per_sm=6
  <CTA alloc> : sm_idx=22 sid=7 max_cta_per_sm=6
  <CTA alloc> : sm_idx=23 sid=8 max_cta_per_sm=6
  <CTA alloc> : sm_idx=24 sid=9 max_cta_per_sm=6
  <CTA alloc> : sm_idx=25 sid=10 max_cta_per_sm=6
  <CTA alloc> : sm_idx=26 sid=11 max_cta_per_sm=6
  <CTA alloc> : sm_idx=27 sid=12 max_cta_per_sm=6
  <CTA alloc> : sm_idx=28 sid=13 max_cta_per_sm=6
  <CTA alloc> : sm_idx=29 sid=14 max_cta_per_sm=6
  <CTA alloc> : sm_idx=30 sid=0 max_cta_per_sm=6
  <CTA alloc> : sm_idx=31 sid=1 max_cta_per_sm=6
  <CTA alloc> : sm_idx=32 sid=2 max_cta_per_sm=6
  <CTA alloc> : sm_idx=33 sid=3 max_cta_per_sm=6
  <CTA alloc> : sm_idx=34 sid=4 max_cta_per_sm=6
  <CTA alloc> : sm_idx=35 sid=5 max_cta_per_sm=6
  <CTA alloc> : sm_idx=36 sid=6 max_cta_per_sm=6
  <CTA alloc> : sm_idx=37 sid=7 max_cta_per_sm=6
  <CTA alloc> : sm_idx=38 sid=8 max_cta_per_sm=6
  <CTA alloc> : sm_idx=39 sid=9 max_cta_per_sm=6
  <CTA alloc> : sm_idx=40 sid=10 max_cta_per_sm=6
  <CTA alloc> : sm_idx=41 sid=11 max_cta_per_sm=6
  <CTA alloc> : sm_idx=42 sid=12 max_cta_per_sm=6
  <CTA alloc> : sm_idx=43 sid=13 max_cta_per_sm=6
  <CTA alloc> : sm_idx=44 sid=14 max_cta_per_sm=6
  <CTA alloc> : sm_idx=45 sid=0 max_cta_per_sm=6
  <CTA alloc> : sm_idx=46 sid=1 max_cta_per_sm=6
  <CTA alloc> : sm_idx=47 sid=2 max_cta_per_sm=6
  <CTA alloc> : sm_idx=48 sid=3 max_cta_per_sm=6
  <CTA alloc> : sm_idx=49 sid=4 max_cta_per_sm=6
  <CTA alloc> : sm_idx=50 sid=5 max_cta_per_sm=6
  <CTA alloc> : sm_idx=51 sid=6 max_cta_per_sm=6
  <CTA alloc> : sm_idx=52 sid=7 max_cta_per_sm=6
  <CTA alloc> : sm_idx=53 sid=8 max_cta_per_sm=6
  <CTA alloc> : sm_idx=54 sid=9 max_cta_per_sm=6
  <CTA alloc> : sm_idx=55 sid=10 max_cta_per_sm=6
  <CTA alloc> : sm_idx=56 sid=11 max_cta_per_sm=6
  <CTA alloc> : sm_idx=57 sid=12 max_cta_per_sm=6
  <CTA alloc> : sm_idx=58 sid=13 max_cta_per_sm=6
  <CTA alloc> : sm_idx=59 sid=14 max_cta_per_sm=6
  <CTA alloc> : sm_idx=60 sid=0 max_cta_per_sm=6
  <CTA alloc> : sm_idx=61 sid=1 max_cta_per_sm=6
  <CTA alloc> : sm_idx=62 sid=2 max_cta_per_sm=6
  <CTA alloc> : sm_idx=63 sid=3 max_cta_per_sm=6
  <CTA alloc> : sm_idx=64 sid=4 max_cta_per_sm=6
  <CTA alloc> : sm_idx=65 sid=5 max_cta_per_sm=6
  <CTA alloc> : sm_idx=66 sid=6 max_cta_per_sm=6
  <CTA alloc> : sm_idx=67 sid=7 max_cta_per_sm=6
  <CTA alloc> : sm_idx=68 sid=8 max_cta_per_sm=6
  <CTA alloc> : sm_idx=69 sid=9 max_cta_per_sm=6
  <CTA alloc> : sm_idx=70 sid=10 max_cta_per_sm=6
  <CTA alloc> : sm_idx=71 sid=11 max_cta_per_sm=6
  <CTA alloc> : sm_idx=72 sid=12 max_cta_per_sm=6
  <CTA alloc> : sm_idx=73 sid=13 max_cta_per_sm=6
  <CTA alloc> : sm_idx=74 sid=14 max_cta_per_sm=6
  <CTA alloc> : sm_idx=75 sid=0 max_cta_per_sm=6
  <CTA alloc> : sm_idx=76 sid=1 max_cta_per_sm=6
  <CTA alloc> : sm_idx=77 sid=2 max_cta_per_sm=6
  <CTA alloc> : sm_idx=78 sid=3 max_cta_per_sm=6
  <CTA alloc> : sm_idx=79 sid=4 max_cta_per_sm=6
  <CTA alloc> : sm_idx=80 sid=5 max_cta_per_sm=6
  <CTA alloc> : sm_idx=81 sid=6 max_cta_per_sm=6
  <CTA alloc> : sm_idx=82 sid=7 max_cta_per_sm=6
  <CTA alloc> : sm_idx=83 sid=8 max_cta_per_sm=6
  <CTA alloc> : sm_idx=84 sid=9 max_cta_per_sm=6
  <CTA alloc> : sm_idx=85 sid=10 max_cta_per_sm=6
  <CTA alloc> : sm_idx=86 sid=11 max_cta_per_sm=6
  <CTA alloc> : sm_idx=87 sid=12 max_cta_per_sm=6
  <CTA alloc> : sm_idx=88 sid=13 max_cta_per_sm=6
  <CTA alloc> : sm_idx=89 sid=14 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 8
w00 0 11111111111111111111111111111111 pc: 0x008 rp: ---- tp: N cd:  1  euclid PC=0x008 (nn_cuda.1.sm_52.ptx:30) ld.param.u64 %rd2, [_Z6euclidP7latLongPfiff_param_1];
w00 0 11111111111111111111111111111111 pc: 0x008 rp: ---- tp: N cd:  1  euclid PC=0x008 (nn_cuda.1.sm_52.ptx:30) ld.param.u64 %rd2, [_Z6euclidP7latLongPfiff_param_1];
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x008 rp: ---- tp: N cd:  1  euclid PC=0x008 (nn_cuda.1.sm_52.ptx:30) ld.param.u64 %rd2, [_Z6euclidP7latLongPfiff_param_1];
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 16
w00 0 11111111111111111111111111111111 pc: 0x010 rp: ---- tp: N cd:  1  euclid PC=0x010 (nn_cuda.1.sm_52.ptx:31) ld.param.u32 %r2, [_Z6euclidP7latLongPfiff_param_2];
w00 0 11111111111111111111111111111111 pc: 0x010 rp: ---- tp: N cd:  1  euclid PC=0x010 (nn_cuda.1.sm_52.ptx:31) ld.param.u32 %r2, [_Z6euclidP7latLongPfiff_param_2];
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x010 rp: ---- tp: N cd:  1  euclid PC=0x010 (nn_cuda.1.sm_52.ptx:31) ld.param.u32 %r2, [_Z6euclidP7latLongPfiff_param_2];
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 24
w00 0 11111111111111111111111111111111 pc: 0x018 rp: ---- tp: N cd:  1  euclid PC=0x018 (nn_cuda.1.sm_52.ptx:32) ld.param.f32 %f1, [_Z6euclidP7latLongPfiff_param_3];
w00 0 11111111111111111111111111111111 pc: 0x018 rp: ---- tp: N cd:  1  euclid PC=0x018 (nn_cuda.1.sm_52.ptx:32) ld.param.f32 %f1, [_Z6euclidP7latLongPfiff_param_3];
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x018 rp: ---- tp: N cd:  1  euclid PC=0x018 (nn_cuda.1.sm_52.ptx:32) ld.param.f32 %f1, [_Z6euclidP7latLongPfiff_param_3];
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 32
w00 0 11111111111111111111111111111111 pc: 0x020 rp: ---- tp: N cd:  1  euclid PC=0x020 (nn_cuda.1.sm_52.ptx:33) ld.param.f32 %f2, [_Z6euclidP7latLongPfiff_param_4];
w00 0 11111111111111111111111111111111 pc: 0x020 rp: ---- tp: N cd:  1  euclid PC=0x020 (nn_cuda.1.sm_52.ptx:33) ld.param.f32 %f2, [_Z6euclidP7latLongPfiff_param_4];
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x020 rp: ---- tp: N cd:  1  euclid PC=0x020 (nn_cuda.1.sm_52.ptx:33) ld.param.f32 %f2, [_Z6euclidP7latLongPfiff_param_4];
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 40
w00 0 11111111111111111111111111111111 pc: 0x028 rp: ---- tp: N cd:  1  euclid PC=0x028 (nn_cuda.1.sm_52.ptx:34) mov.u32 %r3, %ctaid.y;
w00 0 11111111111111111111111111111111 pc: 0x028 rp: ---- tp: N cd:  1  euclid PC=0x028 (nn_cuda.1.sm_52.ptx:34) mov.u32 %r3, %ctaid.y;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x028 rp: ---- tp: N cd:  1  euclid PC=0x028 (nn_cuda.1.sm_52.ptx:34) mov.u32 %r3, %ctaid.y;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 48
w00 0 11111111111111111111111111111111 pc: 0x030 rp: ---- tp: N cd:  1  euclid PC=0x030 (nn_cuda.1.sm_52.ptx:35) mov.u32 %r4, %nctaid.x;
w00 0 11111111111111111111111111111111 pc: 0x030 rp: ---- tp: N cd:  1  euclid PC=0x030 (nn_cuda.1.sm_52.ptx:35) mov.u32 %r4, %nctaid.x;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x030 rp: ---- tp: N cd:  1  euclid PC=0x030 (nn_cuda.1.sm_52.ptx:35) mov.u32 %r4, %nctaid.x;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 56
w00 0 11111111111111111111111111111111 pc: 0x038 rp: ---- tp: N cd:  1  euclid PC=0x038 (nn_cuda.1.sm_52.ptx:36) mov.u32 %r5, %ctaid.x;
w00 0 11111111111111111111111111111111 pc: 0x038 rp: ---- tp: N cd:  1  euclid PC=0x038 (nn_cuda.1.sm_52.ptx:36) mov.u32 %r5, %ctaid.x;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x038 rp: ---- tp: N cd:  1  euclid PC=0x038 (nn_cuda.1.sm_52.ptx:36) mov.u32 %r5, %ctaid.x;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 64
w00 0 11111111111111111111111111111111 pc: 0x040 rp: ---- tp: N cd:  1  euclid PC=0x040 (nn_cuda.1.sm_52.ptx:37) mad.lo.s32 %r6, %r4, %r3, %r5;
w00 0 11111111111111111111111111111111 pc: 0x040 rp: ---- tp: N cd:  1  euclid PC=0x040 (nn_cuda.1.sm_52.ptx:37) mad.lo.s32 %r6, %r4, %r3, %r5;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x040 rp: ---- tp: N cd:  1  euclid PC=0x040 (nn_cuda.1.sm_52.ptx:37) mad.lo.s32 %r6, %r4, %r3, %r5;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 72
w00 0 11111111111111111111111111111111 pc: 0x048 rp: ---- tp: N cd:  1  euclid PC=0x048 (nn_cuda.1.sm_52.ptx:38) mov.u32 %r7, %ntid.x;
w00 0 11111111111111111111111111111111 pc: 0x048 rp: ---- tp: N cd:  1  euclid PC=0x048 (nn_cuda.1.sm_52.ptx:38) mov.u32 %r7, %ntid.x;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x048 rp: ---- tp: N cd:  1  euclid PC=0x048 (nn_cuda.1.sm_52.ptx:38) mov.u32 %r7, %ntid.x;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 80
w00 0 11111111111111111111111111111111 pc: 0x050 rp: ---- tp: N cd:  1  euclid PC=0x050 (nn_cuda.1.sm_52.ptx:39) mov.u32 %r8, %tid.x;
w00 0 11111111111111111111111111111111 pc: 0x050 rp: ---- tp: N cd:  1  euclid PC=0x050 (nn_cuda.1.sm_52.ptx:39) mov.u32 %r8, %tid.x;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x050 rp: ---- tp: N cd:  1  euclid PC=0x050 (nn_cuda.1.sm_52.ptx:39) mov.u32 %r8, %tid.x;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 88
w00 0 11111111111111111111111111111111 pc: 0x058 rp: ---- tp: N cd:  1  euclid PC=0x058 (nn_cuda.1.sm_52.ptx:40) mad.lo.s32 %r1, %r6, %r7, %r8;
w00 0 11111111111111111111111111111111 pc: 0x058 rp: ---- tp: N cd:  1  euclid PC=0x058 (nn_cuda.1.sm_52.ptx:40) mad.lo.s32 %r1, %r6, %r7, %r8;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x058 rp: ---- tp: N cd:  1  euclid PC=0x058 (nn_cuda.1.sm_52.ptx:40) mad.lo.s32 %r1, %r6, %r7, %r8;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 96
w00 0 11111111111111111111111111111111 pc: 0x060 rp: ---- tp: N cd:  1  euclid PC=0x060 (nn_cuda.1.sm_52.ptx:41) setp.ge.s32 %p1, %r1, %r2;
w00 0 11111111111111111111111111111111 pc: 0x060 rp: ---- tp: N cd:  1  euclid PC=0x060 (nn_cuda.1.sm_52.ptx:41) setp.ge.s32 %p1, %r1, %r2;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x060 rp: ---- tp: N cd:  1  euclid PC=0x060 (nn_cuda.1.sm_52.ptx:41) setp.ge.s32 %p1, %r1, %r2;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 104
w00 0 11111111111111111111111111111111 pc: 0x068 rp: ---- tp: N cd:  1  euclid PC=0x068 (nn_cuda.1.sm_52.ptx:42) @%p1 bra $L__BB0_2;
w00 0 11111111111111111111111111111111 pc: 0x068 rp: ---- tp: N cd:  1  euclid PC=0x068 (nn_cuda.1.sm_52.ptx:42) @%p1 bra $L__BB0_2;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x068 rp: ---- tp: N cd:  1  euclid PC=0x068 (nn_cuda.1.sm_52.ptx:42) @%p1 bra $L__BB0_2;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 112
w00 0 11111111111111111111111111111111 pc: 0x070 rp: ---- tp: N cd:  1  euclid PC=0x070 (nn_cuda.1.sm_52.ptx:44) cvta.to.global.u64 %rd3, %rd2;
w00 0 11111111111111111111111111111111 pc: 0x070 rp: ---- tp: N cd:  1  euclid PC=0x070 (nn_cuda.1.sm_52.ptx:44) cvta.to.global.u64 %rd3, %rd2;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x070 rp: ---- tp: N cd:  1  euclid PC=0x070 (nn_cuda.1.sm_52.ptx:44) cvta.to.global.u64 %rd3, %rd2;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 120
w00 0 11111111111111111111111111111111 pc: 0x078 rp: ---- tp: N cd:  1  euclid PC=0x078 (nn_cuda.1.sm_52.ptx:45) mul.wide.s32 %rd4, %r1, 4;
w00 0 11111111111111111111111111111111 pc: 0x078 rp: ---- tp: N cd:  1  euclid PC=0x078 (nn_cuda.1.sm_52.ptx:45) mul.wide.s32 %rd4, %r1, 4;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x078 rp: ---- tp: N cd:  1  euclid PC=0x078 (nn_cuda.1.sm_52.ptx:45) mul.wide.s32 %rd4, %r1, 4;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 128
w00 0 11111111111111111111111111111111 pc: 0x080 rp: ---- tp: N cd:  1  euclid PC=0x080 (nn_cuda.1.sm_52.ptx:46) add.s64 %rd5, %rd3, %rd4;
w00 0 11111111111111111111111111111111 pc: 0x080 rp: ---- tp: N cd:  1  euclid PC=0x080 (nn_cuda.1.sm_52.ptx:46) add.s64 %rd5, %rd3, %rd4;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x080 rp: ---- tp: N cd:  1  euclid PC=0x080 (nn_cuda.1.sm_52.ptx:46) add.s64 %rd5, %rd3, %rd4;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 136
w00 0 11111111111111111111111111111111 pc: 0x088 rp: ---- tp: N cd:  1  euclid PC=0x088 (nn_cuda.1.sm_52.ptx:47) cvta.to.global.u64 %rd6, %rd1;
w00 0 11111111111111111111111111111111 pc: 0x088 rp: ---- tp: N cd:  1  euclid PC=0x088 (nn_cuda.1.sm_52.ptx:47) cvta.to.global.u64 %rd6, %rd1;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x088 rp: ---- tp: N cd:  1  euclid PC=0x088 (nn_cuda.1.sm_52.ptx:47) cvta.to.global.u64 %rd6, %rd1;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 144
w00 0 11111111111111111111111111111111 pc: 0x090 rp: ---- tp: N cd:  1  euclid PC=0x090 (nn_cuda.1.sm_52.ptx:48) mul.wide.s32 %rd7, %r1, 8;
w00 0 11111111111111111111111111111111 pc: 0x090 rp: ---- tp: N cd:  1  euclid PC=0x090 (nn_cuda.1.sm_52.ptx:48) mul.wide.s32 %rd7, %r1, 8;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x090 rp: ---- tp: N cd:  1  euclid PC=0x090 (nn_cuda.1.sm_52.ptx:48) mul.wide.s32 %rd7, %r1, 8;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 152
w00 0 11111111111111111111111111111111 pc: 0x098 rp: ---- tp: N cd:  1  euclid PC=0x098 (nn_cuda.1.sm_52.ptx:49) add.s64 %rd8, %rd6, %rd7;
w00 0 11111111111111111111111111111111 pc: 0x098 rp: ---- tp: N cd:  1  euclid PC=0x098 (nn_cuda.1.sm_52.ptx:49) add.s64 %rd8, %rd6, %rd7;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x098 rp: ---- tp: N cd:  1  euclid PC=0x098 (nn_cuda.1.sm_52.ptx:49) add.s64 %rd8, %rd6, %rd7;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 160
w00 0 11111111111111111111111111111111 pc: 0x0a0 rp: ---- tp: N cd:  1  euclid PC=0x0a0 (nn_cuda.1.sm_52.ptx:50) ld.global.f32 %f3, [%rd8];
w00 0 11111111111111111111111111111111 pc: 0x0a0 rp: ---- tp: N cd:  1  euclid PC=0x0a0 (nn_cuda.1.sm_52.ptx:50) ld.global.f32 %f3, [%rd8];
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0a0 rp: ---- tp: N cd:  1  euclid PC=0x0a0 (nn_cuda.1.sm_52.ptx:50) ld.global.f32 %f3, [%rd8];
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 168
w00 0 11111111111111111111111111111111 pc: 0x0a8 rp: ---- tp: N cd:  1  euclid PC=0x0a8 (nn_cuda.1.sm_52.ptx:51) sub.f32 %f4, %f1, %f3;
w00 0 11111111111111111111111111111111 pc: 0x0a8 rp: ---- tp: N cd:  1  euclid PC=0x0a8 (nn_cuda.1.sm_52.ptx:51) sub.f32 %f4, %f1, %f3;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0a8 rp: ---- tp: N cd:  1  euclid PC=0x0a8 (nn_cuda.1.sm_52.ptx:51) sub.f32 %f4, %f1, %f3;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 176
w00 0 11111111111111111111111111111111 pc: 0x0b0 rp: ---- tp: N cd:  1  euclid PC=0x0b0 (nn_cuda.1.sm_52.ptx:52) ld.global.f32 %f5, [%rd8+4];
w00 0 11111111111111111111111111111111 pc: 0x0b0 rp: ---- tp: N cd:  1  euclid PC=0x0b0 (nn_cuda.1.sm_52.ptx:52) ld.global.f32 %f5, [%rd8+4];
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0b0 rp: ---- tp: N cd:  1  euclid PC=0x0b0 (nn_cuda.1.sm_52.ptx:52) ld.global.f32 %f5, [%rd8+4];
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 184
w00 0 11111111111111111111111111111111 pc: 0x0b8 rp: ---- tp: N cd:  1  euclid PC=0x0b8 (nn_cuda.1.sm_52.ptx:53) sub.f32 %f6, %f2, %f5;
w00 0 11111111111111111111111111111111 pc: 0x0b8 rp: ---- tp: N cd:  1  euclid PC=0x0b8 (nn_cuda.1.sm_52.ptx:53) sub.f32 %f6, %f2, %f5;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0b8 rp: ---- tp: N cd:  1  euclid PC=0x0b8 (nn_cuda.1.sm_52.ptx:53) sub.f32 %f6, %f2, %f5;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 192
w00 0 11111111111111111111111111111111 pc: 0x0c0 rp: ---- tp: N cd:  1  euclid PC=0x0c0 (nn_cuda.1.sm_52.ptx:54) mul.f32 %f7, %f6, %f6;
w00 0 11111111111111111111111111111111 pc: 0x0c0 rp: ---- tp: N cd:  1  euclid PC=0x0c0 (nn_cuda.1.sm_52.ptx:54) mul.f32 %f7, %f6, %f6;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0c0 rp: ---- tp: N cd:  1  euclid PC=0x0c0 (nn_cuda.1.sm_52.ptx:54) mul.f32 %f7, %f6, %f6;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 200
w00 0 11111111111111111111111111111111 pc: 0x0c8 rp: ---- tp: N cd:  1  euclid PC=0x0c8 (nn_cuda.1.sm_52.ptx:55) fma.rn.f32 %f8, %f4, %f4, %f7;
w00 0 11111111111111111111111111111111 pc: 0x0c8 rp: ---- tp: N cd:  1  euclid PC=0x0c8 (nn_cuda.1.sm_52.ptx:55) fma.rn.f32 %f8, %f4, %f4, %f7;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0c8 rp: ---- tp: N cd:  1  euclid PC=0x0c8 (nn_cuda.1.sm_52.ptx:55) fma.rn.f32 %f8, %f4, %f4, %f7;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 208
w00 0 11111111111111111111111111111111 pc: 0x0d0 rp: ---- tp: N cd:  1  euclid PC=0x0d0 (nn_cuda.1.sm_52.ptx:56) sqrt.rn.f32 %f9, %f8;
w00 0 11111111111111111111111111111111 pc: 0x0d0 rp: ---- tp: N cd:  1  euclid PC=0x0d0 (nn_cuda.1.sm_52.ptx:56) sqrt.rn.f32 %f9, %f8;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0d0 rp: ---- tp: N cd:  1  euclid PC=0x0d0 (nn_cuda.1.sm_52.ptx:56) sqrt.rn.f32 %f9, %f8;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 216
w00 0 11111111111111111111111111111111 pc: 0x0d8 rp: ---- tp: N cd:  1  euclid PC=0x0d8 (nn_cuda.1.sm_52.ptx:57) st.global.f32 [%rd5], %f9;
w00 0 11111111111111111111111111111111 pc: 0x0d8 rp: ---- tp: N cd:  1  euclid PC=0x0d8 (nn_cuda.1.sm_52.ptx:57) st.global.f32 [%rd5], %f9;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0d8 rp: ---- tp: N cd:  1  euclid PC=0x0d8 (nn_cuda.1.sm_52.ptx:57) st.global.f32 [%rd5], %f9;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 224
w00 0 11111111111111111111111111111111 pc: 0x0e0 rp: ---- tp: N cd:  1  euclid PC=0x0e0 (nn_cuda.1.sm_52.ptx:60) ret;
w00 0 11111111111111111111111111111111 pc: 0x0e0 rp: ---- tp: N cd:  1  euclid PC=0x0e0 (nn_cuda.1.sm_52.ptx:60) ret;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0e0 rp: ---- tp: N cd:  1  euclid PC=0x0e0 (nn_cuda.1.sm_52.ptx:60) ret;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] pop back 
[Stack after updated]:
  <CTA realloc> : sm_idx=5 sid=5 max_cta_per_sm=6
  <CTA realloc> : sm_idx=20 sid=5 max_cta_per_sm=6
  <CTA realloc> : sm_idx=2 sid=2 max_cta_per_sm=6
  <CTA realloc> : sm_idx=17 sid=2 max_cta_per_sm=6
  <CTA realloc> : sm_idx=6 sid=6 max_cta_per_sm=6
  <CTA realloc> : sm_idx=35 sid=5 max_cta_per_sm=6
  <CTA realloc> : sm_idx=14 sid=14 max_cta_per_sm=6
  <CTA realloc> : sm_idx=4 sid=4 max_cta_per_sm=6
  <CTA realloc> : sm_idx=21 sid=6 max_cta_per_sm=6
  <CTA realloc> : sm_idx=8 sid=8 max_cta_per_sm=6
  <CTA realloc> : sm_idx=0 sid=0 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 8
w00 0 11111111111111111111111111111111 pc: 0x008 rp: ---- tp: N cd:  1  euclid PC=0x008 (nn_cuda.1.sm_52.ptx:30) ld.param.u64 %rd2, [_Z6euclidP7latLongPfiff_param_1];
w00 0 11111111111111111111111111111111 pc: 0x008 rp: ---- tp: N cd:  1  euclid PC=0x008 (nn_cuda.1.sm_52.ptx:30) ld.param.u64 %rd2, [_Z6euclidP7latLongPfiff_param_1];
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x008 rp: ---- tp: N cd:  1  euclid PC=0x008 (nn_cuda.1.sm_52.ptx:30) ld.param.u64 %rd2, [_Z6euclidP7latLongPfiff_param_1];
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 16
w00 0 11111111111111111111111111111111 pc: 0x010 rp: ---- tp: N cd:  1  euclid PC=0x010 (nn_cuda.1.sm_52.ptx:31) ld.param.u32 %r2, [_Z6euclidP7latLongPfiff_param_2];
w00 0 11111111111111111111111111111111 pc: 0x010 rp: ---- tp: N cd:  1  euclid PC=0x010 (nn_cuda.1.sm_52.ptx:31) ld.param.u32 %r2, [_Z6euclidP7latLongPfiff_param_2];
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x010 rp: ---- tp: N cd:  1  euclid PC=0x010 (nn_cuda.1.sm_52.ptx:31) ld.param.u32 %r2, [_Z6euclidP7latLongPfiff_param_2];
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 24
w00 0 11111111111111111111111111111111 pc: 0x018 rp: ---- tp: N cd:  1  euclid PC=0x018 (nn_cuda.1.sm_52.ptx:32) ld.param.f32 %f1, [_Z6euclidP7latLongPfiff_param_3];
w00 0 11111111111111111111111111111111 pc: 0x018 rp: ---- tp: N cd:  1  euclid PC=0x018 (nn_cuda.1.sm_52.ptx:32) ld.param.f32 %f1, [_Z6euclidP7latLongPfiff_param_3];
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x018 rp: ---- tp: N cd:  1  euclid PC=0x018 (nn_cuda.1.sm_52.ptx:32) ld.param.f32 %f1, [_Z6euclidP7latLongPfiff_param_3];
  <CTA realloc> : sm_idx=3 sid=3 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 32
w00 0 11111111111111111111111111111111 pc: 0x020 rp: ---- tp: N cd:  1  euclid PC=0x020 (nn_cuda.1.sm_52.ptx:33) ld.param.f32 %f2, [_Z6euclidP7latLongPfiff_param_4];
w00 0 11111111111111111111111111111111 pc: 0x020 rp: ---- tp: N cd:  1  euclid PC=0x020 (nn_cuda.1.sm_52.ptx:33) ld.param.f32 %f2, [_Z6euclidP7latLongPfiff_param_4];
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x020 rp: ---- tp: N cd:  1  euclid PC=0x020 (nn_cuda.1.sm_52.ptx:33) ld.param.f32 %f2, [_Z6euclidP7latLongPfiff_param_4];
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 40
w00 0 11111111111111111111111111111111 pc: 0x028 rp: ---- tp: N cd:  1  euclid PC=0x028 (nn_cuda.1.sm_52.ptx:34) mov.u32 %r3, %ctaid.y;
w00 0 11111111111111111111111111111111 pc: 0x028 rp: ---- tp: N cd:  1  euclid PC=0x028 (nn_cuda.1.sm_52.ptx:34) mov.u32 %r3, %ctaid.y;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x028 rp: ---- tp: N cd:  1  euclid PC=0x028 (nn_cuda.1.sm_52.ptx:34) mov.u32 %r3, %ctaid.y;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 48
w00 0 11111111111111111111111111111111 pc: 0x030 rp: ---- tp: N cd:  1  euclid PC=0x030 (nn_cuda.1.sm_52.ptx:35) mov.u32 %r4, %nctaid.x;
w00 0 11111111111111111111111111111111 pc: 0x030 rp: ---- tp: N cd:  1  euclid PC=0x030 (nn_cuda.1.sm_52.ptx:35) mov.u32 %r4, %nctaid.x;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x030 rp: ---- tp: N cd:  1  euclid PC=0x030 (nn_cuda.1.sm_52.ptx:35) mov.u32 %r4, %nctaid.x;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 56
w00 0 11111111111111111111111111111111 pc: 0x038 rp: ---- tp: N cd:  1  euclid PC=0x038 (nn_cuda.1.sm_52.ptx:36) mov.u32 %r5, %ctaid.x;
w00 0 11111111111111111111111111111111 pc: 0x038 rp: ---- tp: N cd:  1  euclid PC=0x038 (nn_cuda.1.sm_52.ptx:36) mov.u32 %r5, %ctaid.x;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x038 rp: ---- tp: N cd:  1  euclid PC=0x038 (nn_cuda.1.sm_52.ptx:36) mov.u32 %r5, %ctaid.x;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 64
w00 0 11111111111111111111111111111111 pc: 0x040 rp: ---- tp: N cd:  1  euclid PC=0x040 (nn_cuda.1.sm_52.ptx:37) mad.lo.s32 %r6, %r4, %r3, %r5;
w00 0 11111111111111111111111111111111 pc: 0x040 rp: ---- tp: N cd:  1  euclid PC=0x040 (nn_cuda.1.sm_52.ptx:37) mad.lo.s32 %r6, %r4, %r3, %r5;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x040 rp: ---- tp: N cd:  1  euclid PC=0x040 (nn_cuda.1.sm_52.ptx:37) mad.lo.s32 %r6, %r4, %r3, %r5;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 72
w00 0 11111111111111111111111111111111 pc: 0x048 rp: ---- tp: N cd:  1  euclid PC=0x048 (nn_cuda.1.sm_52.ptx:38) mov.u32 %r7, %ntid.x;
w00 0 11111111111111111111111111111111 pc: 0x048 rp: ---- tp: N cd:  1  euclid PC=0x048 (nn_cuda.1.sm_52.ptx:38) mov.u32 %r7, %ntid.x;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x048 rp: ---- tp: N cd:  1  euclid PC=0x048 (nn_cuda.1.sm_52.ptx:38) mov.u32 %r7, %ntid.x;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 80
w00 0 11111111111111111111111111111111 pc: 0x050 rp: ---- tp: N cd:  1  euclid PC=0x050 (nn_cuda.1.sm_52.ptx:39) mov.u32 %r8, %tid.x;
w00 0 11111111111111111111111111111111 pc: 0x050 rp: ---- tp: N cd:  1  euclid PC=0x050 (nn_cuda.1.sm_52.ptx:39) mov.u32 %r8, %tid.x;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x050 rp: ---- tp: N cd:  1  euclid PC=0x050 (nn_cuda.1.sm_52.ptx:39) mov.u32 %r8, %tid.x;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 88
w00 0 11111111111111111111111111111111 pc: 0x058 rp: ---- tp: N cd:  1  euclid PC=0x058 (nn_cuda.1.sm_52.ptx:40) mad.lo.s32 %r1, %r6, %r7, %r8;
w00 0 11111111111111111111111111111111 pc: 0x058 rp: ---- tp: N cd:  1  euclid PC=0x058 (nn_cuda.1.sm_52.ptx:40) mad.lo.s32 %r1, %r6, %r7, %r8;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x058 rp: ---- tp: N cd:  1  euclid PC=0x058 (nn_cuda.1.sm_52.ptx:40) mad.lo.s32 %r1, %r6, %r7, %r8;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 96
w00 0 11111111111111111111111111111111 pc: 0x060 rp: ---- tp: N cd:  1  euclid PC=0x060 (nn_cuda.1.sm_52.ptx:41) setp.ge.s32 %p1, %r1, %r2;
w00 0 11111111111111111111111111111111 pc: 0x060 rp: ---- tp: N cd:  1  euclid PC=0x060 (nn_cuda.1.sm_52.ptx:41) setp.ge.s32 %p1, %r1, %r2;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x060 rp: ---- tp: N cd:  1  euclid PC=0x060 (nn_cuda.1.sm_52.ptx:41) setp.ge.s32 %p1, %r1, %r2;
  <CTA realloc> : sm_idx=18 sid=3 max_cta_per_sm=6
  <CTA realloc> : sm_idx=9 sid=9 max_cta_per_sm=6
  <CTA realloc> : sm_idx=1 sid=1 max_cta_per_sm=6
  <CTA realloc> : sm_idx=11 sid=11 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 104
w00 0 11111111111111111111111111111111 pc: 0x068 rp: ---- tp: N cd:  1  euclid PC=0x068 (nn_cuda.1.sm_52.ptx:42) @%p1 bra $L__BB0_2;
w00 0 11111111111111111111111111111111 pc: 0x068 rp: ---- tp: N cd:  1  euclid PC=0x068 (nn_cuda.1.sm_52.ptx:42) @%p1 bra $L__BB0_2;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x068 rp: ---- tp: N cd:  1  euclid PC=0x068 (nn_cuda.1.sm_52.ptx:42) @%p1 bra $L__BB0_2;
  <CTA realloc> : sm_idx=7 sid=7 max_cta_per_sm=6
  <CTA realloc> : sm_idx=32 sid=2 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 112
w00 0 11111111111111111111111111111111 pc: 0x070 rp: ---- tp: N cd:  1  euclid PC=0x070 (nn_cuda.1.sm_52.ptx:44) cvta.to.global.u64 %rd3, %rd2;
w00 0 11111111111111111111111111111111 pc: 0x070 rp: ---- tp: N cd:  1  euclid PC=0x070 (nn_cuda.1.sm_52.ptx:44) cvta.to.global.u64 %rd3, %rd2;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x070 rp: ---- tp: N cd:  1  euclid PC=0x070 (nn_cuda.1.sm_52.ptx:44) cvta.to.global.u64 %rd3, %rd2;
  <CTA realloc> : sm_idx=15 sid=0 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 120
w00 0 11111111111111111111111111111111 pc: 0x078 rp: ---- tp: N cd:  1  euclid PC=0x078 (nn_cuda.1.sm_52.ptx:45) mul.wide.s32 %rd4, %r1, 4;
w00 0 11111111111111111111111111111111 pc: 0x078 rp: ---- tp: N cd:  1  euclid PC=0x078 (nn_cuda.1.sm_52.ptx:45) mul.wide.s32 %rd4, %r1, 4;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x078 rp: ---- tp: N cd:  1  euclid PC=0x078 (nn_cuda.1.sm_52.ptx:45) mul.wide.s32 %rd4, %r1, 4;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 128
w00 0 11111111111111111111111111111111 pc: 0x080 rp: ---- tp: N cd:  1  euclid PC=0x080 (nn_cuda.1.sm_52.ptx:46) add.s64 %rd5, %rd3, %rd4;
w00 0 11111111111111111111111111111111 pc: 0x080 rp: ---- tp: N cd:  1  euclid PC=0x080 (nn_cuda.1.sm_52.ptx:46) add.s64 %rd5, %rd3, %rd4;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x080 rp: ---- tp: N cd:  1  euclid PC=0x080 (nn_cuda.1.sm_52.ptx:46) add.s64 %rd5, %rd3, %rd4;
  <CTA realloc> : sm_idx=19 sid=4 max_cta_per_sm=6
  <CTA realloc> : sm_idx=10 sid=10 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 136
w00 0 11111111111111111111111111111111 pc: 0x088 rp: ---- tp: N cd:  1  euclid PC=0x088 (nn_cuda.1.sm_52.ptx:47) cvta.to.global.u64 %rd6, %rd1;
w00 0 11111111111111111111111111111111 pc: 0x088 rp: ---- tp: N cd:  1  euclid PC=0x088 (nn_cuda.1.sm_52.ptx:47) cvta.to.global.u64 %rd6, %rd1;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x088 rp: ---- tp: N cd:  1  euclid PC=0x088 (nn_cuda.1.sm_52.ptx:47) cvta.to.global.u64 %rd6, %rd1;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 144
w00 0 11111111111111111111111111111111 pc: 0x090 rp: ---- tp: N cd:  1  euclid PC=0x090 (nn_cuda.1.sm_52.ptx:48) mul.wide.s32 %rd7, %r1, 8;
w00 0 11111111111111111111111111111111 pc: 0x090 rp: ---- tp: N cd:  1  euclid PC=0x090 (nn_cuda.1.sm_52.ptx:48) mul.wide.s32 %rd7, %r1, 8;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x090 rp: ---- tp: N cd:  1  euclid PC=0x090 (nn_cuda.1.sm_52.ptx:48) mul.wide.s32 %rd7, %r1, 8;
  <CTA realloc> : sm_idx=36 sid=6 max_cta_per_sm=6
  <CTA realloc> : sm_idx=16 sid=1 max_cta_per_sm=6
  <CTA realloc> : sm_idx=23 sid=8 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 152
w00 0 11111111111111111111111111111111 pc: 0x098 rp: ---- tp: N cd:  1  euclid PC=0x098 (nn_cuda.1.sm_52.ptx:49) add.s64 %rd8, %rd6, %rd7;
w00 0 11111111111111111111111111111111 pc: 0x098 rp: ---- tp: N cd:  1  euclid PC=0x098 (nn_cuda.1.sm_52.ptx:49) add.s64 %rd8, %rd6, %rd7;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x098 rp: ---- tp: N cd:  1  euclid PC=0x098 (nn_cuda.1.sm_52.ptx:49) add.s64 %rd8, %rd6, %rd7;
  <CTA realloc> : sm_idx=33 sid=3 max_cta_per_sm=6
  <CTA realloc> : sm_idx=22 sid=7 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 160
w00 0 11111111111111111111111111111111 pc: 0x0a0 rp: ---- tp: N cd:  1  euclid PC=0x0a0 (nn_cuda.1.sm_52.ptx:50) ld.global.f32 %f3, [%rd8];
w00 0 11111111111111111111111111111111 pc: 0x0a0 rp: ---- tp: N cd:  1  euclid PC=0x0a0 (nn_cuda.1.sm_52.ptx:50) ld.global.f32 %f3, [%rd8];
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0a0 rp: ---- tp: N cd:  1  euclid PC=0x0a0 (nn_cuda.1.sm_52.ptx:50) ld.global.f32 %f3, [%rd8];
  <CTA realloc> : sm_idx=29 sid=14 max_cta_per_sm=6
  <CTA realloc> : sm_idx=44 sid=14 max_cta_per_sm=6
  <CTA realloc> : sm_idx=24 sid=9 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 168
w00 0 11111111111111111111111111111111 pc: 0x0a8 rp: ---- tp: N cd:  1  euclid PC=0x0a8 (nn_cuda.1.sm_52.ptx:51) sub.f32 %f4, %f1, %f3;
w00 0 11111111111111111111111111111111 pc: 0x0a8 rp: ---- tp: N cd:  1  euclid PC=0x0a8 (nn_cuda.1.sm_52.ptx:51) sub.f32 %f4, %f1, %f3;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0a8 rp: ---- tp: N cd:  1  euclid PC=0x0a8 (nn_cuda.1.sm_52.ptx:51) sub.f32 %f4, %f1, %f3;
  <CTA realloc> : sm_idx=38 sid=8 max_cta_per_sm=6
  <CTA realloc> : sm_idx=25 sid=10 max_cta_per_sm=6
  <CTA realloc> : sm_idx=26 sid=11 max_cta_per_sm=6
  <CTA realloc> : sm_idx=31 sid=1 max_cta_per_sm=6
  <CTA realloc> : sm_idx=12 sid=12 max_cta_per_sm=6
  <CTA realloc> : sm_idx=50 sid=5 max_cta_per_sm=6
  <CTA realloc> : sm_idx=39 sid=9 max_cta_per_sm=6
  <CTA realloc> : sm_idx=30 sid=0 max_cta_per_sm=6
  <CTA realloc> : sm_idx=34 sid=4 max_cta_per_sm=6
  <CTA realloc> : sm_idx=41 sid=11 max_cta_per_sm=6
  <CTA realloc> : sm_idx=47 sid=2 max_cta_per_sm=6
  <CTA realloc> : sm_idx=53 sid=8 max_cta_per_sm=6
  <CTA realloc> : sm_idx=65 sid=5 max_cta_per_sm=6
  <CTA realloc> : sm_idx=54 sid=9 max_cta_per_sm=6
  <CTA realloc> : sm_idx=40 sid=10 max_cta_per_sm=6
  <CTA realloc> : sm_idx=46 sid=1 max_cta_per_sm=6
  <CTA realloc> : sm_idx=27 sid=12 max_cta_per_sm=6
  <CTA realloc> : sm_idx=13 sid=13 max_cta_per_sm=6
  <CTA realloc> : sm_idx=45 sid=0 max_cta_per_sm=6
  <CTA realloc> : sm_idx=62 sid=2 max_cta_per_sm=6
  <CTA realloc> : sm_idx=48 sid=3 max_cta_per_sm=6
  <CTA realloc> : sm_idx=49 sid=4 max_cta_per_sm=6
  <CTA realloc> : sm_idx=80 sid=5 max_cta_per_sm=6
  <CTA realloc> : sm_idx=37 sid=7 max_cta_per_sm=6
  <CTA realloc> : sm_idx=56 sid=11 max_cta_per_sm=6
  <CTA realloc> : sm_idx=28 sid=13 max_cta_per_sm=6
  <CTA realloc> : sm_idx=63 sid=3 max_cta_per_sm=6
  <CTA realloc> : sm_idx=59 sid=14 max_cta_per_sm=6
  <CTA realloc> : sm_idx=51 sid=6 max_cta_per_sm=6
  <CTA realloc> : sm_idx=52 sid=7 max_cta_per_sm=6
  <CTA realloc> : sm_idx=42 sid=12 max_cta_per_sm=6
  <CTA realloc> : sm_idx=68 sid=8 max_cta_per_sm=6
  <CTA realloc> : sm_idx=43 sid=13 max_cta_per_sm=6
  <CTA realloc> : sm_idx=55 sid=10 max_cta_per_sm=6
  <CTA realloc> : sm_idx=66 sid=6 max_cta_per_sm=6
  <CTA realloc> : sm_idx=57 sid=12 max_cta_per_sm=6
  <CTA realloc> : sm_idx=58 sid=13 max_cta_per_sm=6
  <CTA realloc> : sm_idx=77 sid=2 max_cta_per_sm=6
  <CTA realloc> : sm_idx=60 sid=0 max_cta_per_sm=6
  <CTA realloc> : sm_idx=81 sid=6 max_cta_per_sm=6
  <CTA realloc> : sm_idx=71 sid=11 max_cta_per_sm=6
  <CTA realloc> : sm_idx=69 sid=9 max_cta_per_sm=6
  <CTA realloc> : sm_idx=74 sid=14 max_cta_per_sm=6
  <CTA realloc> : sm_idx=64 sid=4 max_cta_per_sm=6
  <CTA realloc> : sm_idx=78 sid=3 max_cta_per_sm=6
  <CTA realloc> : sm_idx=72 sid=12 max_cta_per_sm=6
  <CTA realloc> : sm_idx=83 sid=8 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 176
w00 0 11111111111111111111111111111111 pc: 0x0b0 rp: ---- tp: N cd:  1  euclid PC=0x0b0 (nn_cuda.1.sm_52.ptx:52) ld.global.f32 %f5, [%rd8+4];
w00 0 11111111111111111111111111111111 pc: 0x0b0 rp: ---- tp: N cd:  1  euclid PC=0x0b0 (nn_cuda.1.sm_52.ptx:52) ld.global.f32 %f5, [%rd8+4];
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0b0 rp: ---- tp: N cd:  1  euclid PC=0x0b0 (nn_cuda.1.sm_52.ptx:52) ld.global.f32 %f5, [%rd8+4];
  <CTA realloc> : sm_idx=75 sid=0 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 184
w00 0 11111111111111111111111111111111 pc: 0x0b8 rp: ---- tp: N cd:  1  euclid PC=0x0b8 (nn_cuda.1.sm_52.ptx:53) sub.f32 %f6, %f2, %f5;
w00 0 11111111111111111111111111111111 pc: 0x0b8 rp: ---- tp: N cd:  1  euclid PC=0x0b8 (nn_cuda.1.sm_52.ptx:53) sub.f32 %f6, %f2, %f5;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0b8 rp: ---- tp: N cd:  1  euclid PC=0x0b8 (nn_cuda.1.sm_52.ptx:53) sub.f32 %f6, %f2, %f5;
  <CTA realloc> : sm_idx=67 sid=7 max_cta_per_sm=6
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 192
w00 0 11111111111111111111111111111111 pc: 0x0c0 rp: ---- tp: N cd:  1  euclid PC=0x0c0 (nn_cuda.1.sm_52.ptx:54) mul.f32 %f7, %f6, %f6;
w00 0 11111111111111111111111111111111 pc: 0x0c0 rp: ---- tp: N cd:  1  euclid PC=0x0c0 (nn_cuda.1.sm_52.ptx:54) mul.f32 %f7, %f6, %f6;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0c0 rp: ---- tp: N cd:  1  euclid PC=0x0c0 (nn_cuda.1.sm_52.ptx:54) mul.f32 %f7, %f6, %f6;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 200
w00 0 11111111111111111111111111111111 pc: 0x0c8 rp: ---- tp: N cd:  1  euclid PC=0x0c8 (nn_cuda.1.sm_52.ptx:55) fma.rn.f32 %f8, %f4, %f4, %f7;
w00 0 11111111111111111111111111111111 pc: 0x0c8 rp: ---- tp: N cd:  1  euclid PC=0x0c8 (nn_cuda.1.sm_52.ptx:55) fma.rn.f32 %f8, %f4, %f4, %f7;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0c8 rp: ---- tp: N cd:  1  euclid PC=0x0c8 (nn_cuda.1.sm_52.ptx:55) fma.rn.f32 %f8, %f4, %f4, %f7;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 208
w00 0 11111111111111111111111111111111 pc: 0x0d0 rp: ---- tp: N cd:  1  euclid PC=0x0d0 (nn_cuda.1.sm_52.ptx:56) sqrt.rn.f32 %f9, %f8;
w00 0 11111111111111111111111111111111 pc: 0x0d0 rp: ---- tp: N cd:  1  euclid PC=0x0d0 (nn_cuda.1.sm_52.ptx:56) sqrt.rn.f32 %f9, %f8;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0d0 rp: ---- tp: N cd:  1  euclid PC=0x0d0 (nn_cuda.1.sm_52.ptx:56) sqrt.rn.f32 %f9, %f8;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 216
w00 0 11111111111111111111111111111111 pc: 0x0d8 rp: ---- tp: N cd:  1  euclid PC=0x0d8 (nn_cuda.1.sm_52.ptx:57) st.global.f32 [%rd5], %f9;
w00 0 11111111111111111111111111111111 pc: 0x0d8 rp: ---- tp: N cd:  1  euclid PC=0x0d8 (nn_cuda.1.sm_52.ptx:57) st.global.f32 [%rd5], %f9;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0d8 rp: ---- tp: N cd:  1  euclid PC=0x0d8 (nn_cuda.1.sm_52.ptx:57) st.global.f32 [%rd5], %f9;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] update back m_pc to tmp_next_pc 224
w00 0 11111111111111111111111111111111 pc: 0x0e0 rp: ---- tp: N cd:  1  euclid PC=0x0e0 (nn_cuda.1.sm_52.ptx:60) ret;
w00 0 11111111111111111111111111111111 pc: 0x0e0 rp: ---- tp: N cd:  1  euclid PC=0x0e0 (nn_cuda.1.sm_52.ptx:60) ret;
[Jiayi Test Stack] push new entry
[Jiayi Test Stack] pop back 
[Stack after updated]:
w00 0 11111111111111111111111111111111 pc: 0x0e0 rp: ---- tp: N cd:  1  euclid PC=0x0e0 (nn_cuda.1.sm_52.ptx:60) ret;
[Jiayi Test Stack]  simt_stack::update_sid[Jiayi Test Stack] pop back 
[Stack after updated]:
Destroy streams for kernel 1: size 0
[Jiayi Test0]: if (ctx->the_gpgpusim->g_stream_manager->operation(&sim_cycles) && !ctx->the_gpgpusim->g_the_gpu->active())
GPGPU-Sim: ** STOP simulation thread (no work) **
kernel_name = _Z6euclidP7latLongPfiff 
kernel_launch_uid = 1 
gpu_sim_cycle = 3688
gpu_sim_insn = 1201052
gpu_ipc =     325.6649
gpu_tot_sim_cycle = 3688
gpu_tot_sim_insn = 1201052
gpu_tot_ipc =     325.6649
gpu_tot_issued_cta = 168
gpu_occupancy = 85.7809% 
gpu_tot_occupancy = 85.7809% 
max_total_param_size = 0
gpu_stall_dramfull = 2766
gpu_stall_icnt2sh    = 7724
partiton_level_parallism =       1.0995
partiton_level_parallism_total  =       1.0995
partiton_level_parallism_util =       2.2988
partiton_level_parallism_util_total  =       2.2988
L2_BW  =      98.3219 GB/Sec
L2_BW_total  =      98.3219 GB/Sec
gpu_total_sim_rate=70650

========= Core cache stats =========
L1I_cache:
	L1I_total_cache_accesses = 20118
	L1I_total_cache_misses = 960
	L1I_total_cache_miss_rate = 0.0477
	L1I_total_cache_pending_hits = 0
	L1I_total_cache_reservation_fails = 4163
L1D_cache:
	L1D_cache_core[0]: Access = 480, Miss = 288, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 325
	L1D_cache_core[1]: Access = 400, Miss = 240, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 313
	L1D_cache_core[2]: Access = 480, Miss = 288, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 283
	L1D_cache_core[3]: Access = 480, Miss = 288, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 322
	L1D_cache_core[4]: Access = 440, Miss = 264, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 309
	L1D_cache_core[5]: Access = 480, Miss = 288, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 218
	L1D_cache_core[6]: Access = 480, Miss = 288, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 269
	L1D_cache_core[7]: Access = 403, Miss = 242, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 333
	L1D_cache_core[8]: Access = 480, Miss = 288, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 419
	L1D_cache_core[9]: Access = 440, Miss = 264, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 363
	L1D_cache_core[10]: Access = 400, Miss = 240, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 342
	L1D_cache_core[11]: Access = 440, Miss = 264, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 474
	L1D_cache_core[12]: Access = 440, Miss = 264, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 470
	L1D_cache_core[13]: Access = 400, Miss = 240, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 437
	L1D_cache_core[14]: Access = 440, Miss = 264, Miss_rate = 0.600, Pending_hits = 0, Reservation_fails = 392
	L1D_total_cache_accesses = 6683
	L1D_total_cache_misses = 4010
	L1D_total_cache_miss_rate = 0.6000
	L1D_total_cache_pending_hits = 0
	L1D_total_cache_reservation_fails = 5269
	L1D_cache_data_port_util = 0.050
	L1D_cache_fill_port_util = 0.050
L1C_cache:
	L1C_total_cache_accesses = 6720
	L1C_total_cache_misses = 480
	L1C_total_cache_miss_rate = 0.0714
	L1C_total_cache_pending_hits = 0
	L1C_total_cache_reservation_fails = 3389
L1T_cache:
	L1T_total_cache_accesses = 0
	L1T_total_cache_misses = 0
	L1T_total_cache_pending_hits = 0
	L1T_total_cache_reservation_fails = 0

Total_core_cache_stats:
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][HIT] = 2673
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][MISS] = 2673
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][RESERVATION_FAIL] = 5269
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][HIT] = 6240
	Total_core_cache_stats_breakdown[CONST_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][MISS] = 480
	Total_core_cache_stats_breakdown[CONST_ACC_R][RESERVATION_FAIL] = 3389
	Total_core_cache_stats_breakdown[CONST_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][HIT] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][MISS] = 1337
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][HIT] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][MISS] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][HIT] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][MISS] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][HIT] = 19158
	Total_core_cache_stats_breakdown[INST_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][MISS] = 960
	Total_core_cache_stats_breakdown[INST_ACC_R][RESERVATION_FAIL] = 4163
	Total_core_cache_stats_breakdown[INST_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][HIT] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][MISS] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][HIT] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][MISS] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][TOTAL_ACCESS] = 5346
	Total_core_cache_stats_breakdown[CONST_ACC_R][TOTAL_ACCESS] = 6720
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][TOTAL_ACCESS] = 1337
	Total_core_cache_stats_breakdown[INST_ACC_R][TOTAL_ACCESS] = 20118

Total_core_cache_fail_stats:
	Total_core_cache_fail_stats_breakdown[GLOBAL_ACC_R][MSHR_ENRTY_FAIL] = 5269
	Total_core_cache_fail_stats_breakdown[CONST_ACC_R][MSHR_MERGE_ENRTY_FAIL] = 3389
	Total_core_cache_fail_stats_breakdown[INST_ACC_R][MSHR_MERGE_ENRTY_FAIL] = 4163
ctas_completed 168, Shader 0 warp_id issue ditsribution:
warp_id:
0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 
distro:
58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 
gpgpu_n_tot_thrd_icount = 1244096
gpgpu_n_tot_w_icount = 38878
gpgpu_n_stall_shd_mem = 6061
gpgpu_n_mem_read_local = 0
gpgpu_n_mem_write_local = 0
gpgpu_n_mem_read_global = 2673
gpgpu_n_mem_write_global = 1337
gpgpu_n_mem_texture = 0
gpgpu_n_mem_const = 15
gpgpu_n_load_insn  = 85528
gpgpu_n_store_insn = 42764
gpgpu_n_shmem_insn = 0
gpgpu_n_sstarr_insn = 0
gpgpu_n_tex_insn = 0
gpgpu_n_const_mem_insn = 0
gpgpu_n_param_mem_insn = 215040
gpgpu_n_shmem_bkconflict = 0
gpgpu_n_cache_bkconflict = 0
gpgpu_n_intrawarp_mshr_merge = 0
gpgpu_n_cmem_portconflict = 3389
gpgpu_stall_shd_mem[c_mem][resource_stall] = 3389
gpgpu_stall_shd_mem[s_mem][bk_conf] = 0
gpgpu_stall_shd_mem[gl_mem][resource_stall] = 0
gpgpu_stall_shd_mem[gl_mem][coal_stall] = 2672
gpgpu_stall_shd_mem[gl_mem][data_port_stall] = 0
gpu_reg_bank_conflict_stalls = 0
Warp Occupancy Distribution:
Stall:9654	W0_Idle:18346	W0_Scoreboard:40718	W1:0	W2:0	W3:0	W4:0	W5:0	W6:0	W7:0	W8:0	W9:0	W10:0	W11:0	W12:14	W13:0	W14:0	W15:0	W16:0	W17:0	W18:0	W19:0	W20:0	W21:0	W22:0	W23:0	W24:0	W25:0	W26:0	W27:0	W28:0	W29:0	W30:0	W31:0	W32:38864
single_issue_nums: WS0:19446	WS1:19432	
dual_issue_nums: WS0:0	WS1:0	
traffic_breakdown_coretomem[CONST_ACC_R] = 120 {8:15,}
traffic_breakdown_coretomem[GLOBAL_ACC_R] = 21384 {8:2673,}
traffic_breakdown_coretomem[GLOBAL_ACC_W] = 181768 {72:1,136:1336,}
traffic_breakdown_coretomem[INST_ACC_R] = 240 {8:30,}
traffic_breakdown_memtocore[CONST_ACC_R] = 1200 {40:30,}
traffic_breakdown_memtocore[GLOBAL_ACC_R] = 427680 {40:10692,}
traffic_breakdown_memtocore[GLOBAL_ACC_W] = 42768 {8:5346,}
traffic_breakdown_memtocore[INST_ACC_R] = 4800 {40:120,}
maxmflatency = 670 
max_icnt2mem_latency = 611 
maxmrqlatency = 21 
max_icnt2sh_latency = 181 
averagemflatency = 353 
avg_icnt2mem_latency = 21 
avg_mrq_latency = 10 
avg_icnt2sh_latency = 108 
mrq_lat_table:4 	0 	0 	3 	4 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
dq_lat_table:0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
mf_lat_table:0 	0 	0 	0 	0 	0 	0 	4501 	9359 	2208 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
icnt2mem_lat_table:0 	0 	14 	25 	6 	2663 	717 	372 	222 	36 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
icnt2sh_lat_table:0 	0 	333 	373 	507 	1125 	8957 	4773 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
mf_lat_pw_table:0 	0 	0 	0 	0 	0 	0 	2 	4 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
maximum concurrent accesses to same row:
dram[0]:         4         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
maximum service time to same row:
dram[0]:       330         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
average row accesses per activate:
dram[0]:  3.333333      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
dram[1]:      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
dram[2]:      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
dram[3]:      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
dram[4]:      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan       inf      -nan      -nan      -nan 
dram[5]:      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan      -nan 
average row locality = 11/3 = 3.666667
number of total memory accesses made:
dram[0]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
total accesses: 0
min_bank_accesses = 0!
min_chip_accesses = 0!
number of total read accesses:
dram[0]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
total dram reads = 0
min_bank_accesses = 0!
min_chip_accesses = 0!
number of total write accesses:
dram[0]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
total dram writes = 0
min_bank_accesses = 0!
min_chip_accesses = 0!
average mf latency per bank:
dram[0]:     none      none      none      none      none      none      none      none      none      none      none      none      none      none      none      none  
dram[1]:     none      none      none      none      none      none      none      none      none      none      none      none      none      none      none      none  
dram[2]:     none      none      none      none      none      none      none      none      none      none      none      none      none      none      none      none  
dram[3]:     none      none      none      none      none      none      none      none      none      none      none      none      none      none      none      none  
dram[4]:     none      none      none      none      none      none      none      none      none      none      none      none      none      none      none      none  
dram[5]:     none      none      none      none      none      none      none      none      none      none      none      none      none      none      none      none  
maximum mf latency per bank:
dram[0]:        366       503       587       626       586       624       531       622       333       310       408       464       467       535       441       544
dram[1]:        417       467       547       594       578       636       519       595       316       357       407       450       504       533       486       570
dram[2]:        483       561       571       598       570       605       523       530       208       192       235       265       305       341       518       384
dram[3]:        388       538       466       648       500       670       436       652       289       394       356       492       412       587       401       516
dram[4]:        468       574       641       611       662       614       642       571       258       213       318       284       452       439       552       420
dram[5]:        361       525       496       614       556       622       541       627       209       283       336       352       408       452       450       531

Number of Memory Banks Accessed per Memory Operation per Warp (from 0):
0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
Average # of Memory Banks Accessed per Memory Operation per Warp=-nan

position of mrq chosen
0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	

average position of mrq chosen = -nan
Memory Partition 0: 
Cache L2_bank_000:
MSHR contents

Cache L2_bank_001:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[0]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=4867 n_nop=4852 n_act=3 n_pre=2 n_ref_event=13744632839234567870 n_req=10 n_rd=10 n_rd_L2_A=0 n_write=0 n_wr_bk=0 bw_util=0.004109
n_activity=144 dram_eff=0.1389
bk0: 10a 4793i bk1: 0a 4865i bk2: 0a 4865i bk3: 0a 4866i bk4: 0a 4866i bk5: 0a 4867i bk6: 0a 4867i bk7: 0a 4867i bk8: 0a 4867i bk9: 0a 4867i bk10: 0a 4867i bk11: 0a 4867i bk12: 0a 4867i bk13: 0a 4868i bk14: 0a 4868i bk15: 0a 4869i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.700000
Row_Buffer_Locality_read = 0.700000
Row_Buffer_Locality_write = -nan
Bank_Level_Parallism = 1.000000
Bank_Level_Parallism_Col = 1.000000
Bank_Level_Parallism_Ready = 1.000000
write_to_read_ratio_blp_rw_average = 0.000000
GrpLevelPara = 0.000000 

BW Util details:
bwutil = 0.004109 
total_CMD = 4867 
util_bw = 20 
Wasted_Col = 43 
Wasted_Row = 24 
Idle = 4780 

BW Util Bottlenecks: 
RCDc_limit = 36 
RCDWRc_limit = 0 
WTRc_limit = 0 
RTWc_limit = 0 
CCDLc_limit = 7 
rwq = 0 
CCDLc_limit_alone = 7 
WTRc_limit_alone = 0 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 4867 
n_nop = 4852 
Read = 10 
Write = 0 
L2_Alloc = 0 
L2_WB = 0 
n_act = 3 
n_pre = 2 
n_ref = 13744632839234567870 
n_req = 10 
total_req = 10 

Dual Bus Interface Util: 
issued_total_row = 5 
issued_total_col = 10 
Row_Bus_Util =  0.001027 
CoL_Bus_Util = 0.002055 
Either_Row_CoL_Bus_Util = 0.003082 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = 0.000000 
queue_avg = 0.028765 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=3 avg=0.0287652
Memory Partition 1: 
Cache L2_bank_002:
MSHR contents

Cache L2_bank_003:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[1]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=4867 n_nop=4867 n_act=0 n_pre=0 n_ref_event=13744632839234567870 n_req=0 n_rd=0 n_rd_L2_A=0 n_write=0 n_wr_bk=0 bw_util=0
n_activity=0 dram_eff=-nan
bk0: 0a 4867i bk1: 0a 4867i bk2: 0a 4867i bk3: 0a 4867i bk4: 0a 4867i bk5: 0a 4867i bk6: 0a 4867i bk7: 0a 4867i bk8: 0a 4867i bk9: 0a 4867i bk10: 0a 4867i bk11: 0a 4867i bk12: 0a 4867i bk13: 0a 4867i bk14: 0a 4867i bk15: 0a 4867i 

------------------------------------------------------------------------

Row_Buffer_Locality = -nan
Row_Buffer_Locality_read = -nan
Row_Buffer_Locality_write = -nan
Bank_Level_Parallism = -nan
Bank_Level_Parallism_Col = 1.000000
Bank_Level_Parallism_Ready = -nan
write_to_read_ratio_blp_rw_average = 0.000000
GrpLevelPara = 0.000000 

BW Util details:
bwutil = 0.000000 
total_CMD = 4867 
util_bw = 0 
Wasted_Col = 0 
Wasted_Row = 0 
Idle = 4867 

BW Util Bottlenecks: 
RCDc_limit = 0 
RCDWRc_limit = 0 
WTRc_limit = 0 
RTWc_limit = 0 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 0 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 4867 
n_nop = 4867 
Read = 0 
Write = 0 
L2_Alloc = 0 
L2_WB = 0 
n_act = 0 
n_pre = 0 
n_ref = 13744632839234567870 
n_req = 0 
total_req = 0 

Dual Bus Interface Util: 
issued_total_row = 0 
issued_total_col = 0 
Row_Bus_Util =  0.000000 
CoL_Bus_Util = 0.000000 
Either_Row_CoL_Bus_Util = 0.000000 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = -nan 
queue_avg = 0.000000 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=0 avg=0
Memory Partition 2: 
Cache L2_bank_004:
MSHR contents

Cache L2_bank_005:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[2]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=4867 n_nop=4867 n_act=0 n_pre=0 n_ref_event=13744632839234567870 n_req=0 n_rd=0 n_rd_L2_A=0 n_write=0 n_wr_bk=0 bw_util=0
n_activity=0 dram_eff=-nan
bk0: 0a 4867i bk1: 0a 4867i bk2: 0a 4867i bk3: 0a 4867i bk4: 0a 4867i bk5: 0a 4867i bk6: 0a 4867i bk7: 0a 4867i bk8: 0a 4867i bk9: 0a 4867i bk10: 0a 4867i bk11: 0a 4867i bk12: 0a 4867i bk13: 0a 4867i bk14: 0a 4867i bk15: 0a 4867i 

------------------------------------------------------------------------

Row_Buffer_Locality = -nan
Row_Buffer_Locality_read = -nan
Row_Buffer_Locality_write = -nan
Bank_Level_Parallism = -nan
Bank_Level_Parallism_Col = 1.000000
Bank_Level_Parallism_Ready = -nan
write_to_read_ratio_blp_rw_average = 0.000000
GrpLevelPara = 0.000000 

BW Util details:
bwutil = 0.000000 
total_CMD = 4867 
util_bw = 0 
Wasted_Col = 0 
Wasted_Row = 0 
Idle = 4867 

BW Util Bottlenecks: 
RCDc_limit = 0 
RCDWRc_limit = 0 
WTRc_limit = 0 
RTWc_limit = 0 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 0 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 4867 
n_nop = 4867 
Read = 0 
Write = 0 
L2_Alloc = 0 
L2_WB = 0 
n_act = 0 
n_pre = 0 
n_ref = 13744632839234567870 
n_req = 0 
total_req = 0 

Dual Bus Interface Util: 
issued_total_row = 0 
issued_total_col = 0 
Row_Bus_Util =  0.000000 
CoL_Bus_Util = 0.000000 
Either_Row_CoL_Bus_Util = 0.000000 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = -nan 
queue_avg = 0.000000 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=0 avg=0
Memory Partition 3: 
Cache L2_bank_006:
MSHR contents

Cache L2_bank_007:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[3]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=4867 n_nop=4867 n_act=0 n_pre=0 n_ref_event=13744632839234567870 n_req=0 n_rd=0 n_rd_L2_A=0 n_write=0 n_wr_bk=0 bw_util=0
n_activity=0 dram_eff=-nan
bk0: 0a 4867i bk1: 0a 4867i bk2: 0a 4867i bk3: 0a 4867i bk4: 0a 4867i bk5: 0a 4867i bk6: 0a 4867i bk7: 0a 4867i bk8: 0a 4867i bk9: 0a 4867i bk10: 0a 4867i bk11: 0a 4867i bk12: 0a 4867i bk13: 0a 4867i bk14: 0a 4867i bk15: 0a 4867i 

------------------------------------------------------------------------

Row_Buffer_Locality = -nan
Row_Buffer_Locality_read = -nan
Row_Buffer_Locality_write = -nan
Bank_Level_Parallism = -nan
Bank_Level_Parallism_Col = 1.000000
Bank_Level_Parallism_Ready = -nan
write_to_read_ratio_blp_rw_average = 0.000000
GrpLevelPara = 0.000000 

BW Util details:
bwutil = 0.000000 
total_CMD = 4867 
util_bw = 0 
Wasted_Col = 0 
Wasted_Row = 0 
Idle = 4867 

BW Util Bottlenecks: 
RCDc_limit = 0 
RCDWRc_limit = 0 
WTRc_limit = 0 
RTWc_limit = 0 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 0 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 4867 
n_nop = 4867 
Read = 0 
Write = 0 
L2_Alloc = 0 
L2_WB = 0 
n_act = 0 
n_pre = 0 
n_ref = 13744632839234567870 
n_req = 0 
total_req = 0 

Dual Bus Interface Util: 
issued_total_row = 0 
issued_total_col = 0 
Row_Bus_Util =  0.000000 
CoL_Bus_Util = 0.000000 
Either_Row_CoL_Bus_Util = 0.000000 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = -nan 
queue_avg = 0.000000 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=0 avg=0
Memory Partition 4: 
Cache L2_bank_008:
MSHR contents

Cache L2_bank_009:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[4]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=4867 n_nop=4865 n_act=1 n_pre=0 n_ref_event=13744632839234567870 n_req=1 n_rd=1 n_rd_L2_A=0 n_write=0 n_wr_bk=0 bw_util=0.0004109
n_activity=40 dram_eff=0.05
bk0: 0a 4866i bk1: 0a 4867i bk2: 0a 4867i bk3: 0a 4867i bk4: 0a 4867i bk5: 0a 4867i bk6: 0a 4867i bk7: 0a 4867i bk8: 0a 4867i bk9: 0a 4868i bk10: 0a 4868i bk11: 0a 4868i bk12: 1a 4855i bk13: 0a 4866i bk14: 0a 4866i bk15: 0a 4866i 

------------------------------------------------------------------------

Row_Buffer_Locality = 1.000000
Row_Buffer_Locality_read = 1.000000
Row_Buffer_Locality_write = -nan
Bank_Level_Parallism = 1.000000
Bank_Level_Parallism_Col = 1.000000
Bank_Level_Parallism_Ready = 1.000000
write_to_read_ratio_blp_rw_average = 0.000000
GrpLevelPara = 0.000000 

BW Util details:
bwutil = 0.000411 
total_CMD = 4867 
util_bw = 2 
Wasted_Col = 12 
Wasted_Row = 0 
Idle = 4853 

BW Util Bottlenecks: 
RCDc_limit = 12 
RCDWRc_limit = 0 
WTRc_limit = 0 
RTWc_limit = 0 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 0 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 4867 
n_nop = 4865 
Read = 1 
Write = 0 
L2_Alloc = 0 
L2_WB = 0 
n_act = 1 
n_pre = 0 
n_ref = 13744632839234567870 
n_req = 1 
total_req = 1 

Dual Bus Interface Util: 
issued_total_row = 1 
issued_total_col = 1 
Row_Bus_Util =  0.000205 
CoL_Bus_Util = 0.000205 
Either_Row_CoL_Bus_Util = 0.000411 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = 0.000000 
queue_avg = 0.000000 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=0 avg=0
Memory Partition 5: 
Cache L2_bank_010:
MSHR contents

Cache L2_bank_011:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[5]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=4867 n_nop=4867 n_act=0 n_pre=0 n_ref_event=13744632839234567870 n_req=0 n_rd=0 n_rd_L2_A=0 n_write=0 n_wr_bk=0 bw_util=0
n_activity=0 dram_eff=-nan
bk0: 0a 4867i bk1: 0a 4867i bk2: 0a 4867i bk3: 0a 4867i bk4: 0a 4867i bk5: 0a 4867i bk6: 0a 4867i bk7: 0a 4867i bk8: 0a 4867i bk9: 0a 4867i bk10: 0a 4867i bk11: 0a 4867i bk12: 0a 4867i bk13: 0a 4867i bk14: 0a 4867i bk15: 0a 4867i 

------------------------------------------------------------------------

Row_Buffer_Locality = -nan
Row_Buffer_Locality_read = -nan
Row_Buffer_Locality_write = -nan
Bank_Level_Parallism = -nan
Bank_Level_Parallism_Col = 1.000000
Bank_Level_Parallism_Ready = -nan
write_to_read_ratio_blp_rw_average = 0.000000
GrpLevelPara = 0.000000 

BW Util details:
bwutil = 0.000000 
total_CMD = 4867 
util_bw = 0 
Wasted_Col = 0 
Wasted_Row = 0 
Idle = 4867 

BW Util Bottlenecks: 
RCDc_limit = 0 
RCDWRc_limit = 0 
WTRc_limit = 0 
RTWc_limit = 0 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 0 
RTWc_limit_alone = 0 

Commands details: 
total_CMD = 4867 
n_nop = 4867 
Read = 0 
Write = 0 
L2_Alloc = 0 
L2_WB = 0 
n_act = 0 
n_pre = 0 
n_ref = 13744632839234567870 
n_req = 0 
total_req = 0 

Dual Bus Interface Util: 
issued_total_row = 0 
issued_total_col = 0 
Row_Bus_Util =  0.000000 
CoL_Bus_Util = 0.000000 
Either_Row_CoL_Bus_Util = 0.000000 
Issued_on_Two_Bus_Simul_Util = 0.000000 
issued_two_Eff = -nan 
queue_avg = 0.000000 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=0 avg=0

========= L2 cache stats =========
L2_cache_bank[0]: Access = 1494, Miss = 458, Miss_rate = 0.307, Pending_hits = 30, Reservation_fails = 343
L2_cache_bank[1]: Access = 1336, Miss = 448, Miss_rate = 0.335, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[2]: Access = 1338, Miss = 442, Miss_rate = 0.330, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[3]: Access = 1336, Miss = 448, Miss_rate = 0.335, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[4]: Access = 1336, Miss = 440, Miss_rate = 0.329, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[5]: Access = 1336, Miss = 448, Miss_rate = 0.335, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[6]: Access = 1336, Miss = 440, Miss_rate = 0.329, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[7]: Access = 1336, Miss = 448, Miss_rate = 0.335, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[8]: Access = 1332, Miss = 441, Miss_rate = 0.331, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[9]: Access = 1336, Miss = 448, Miss_rate = 0.335, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[10]: Access = 1336, Miss = 448, Miss_rate = 0.335, Pending_hits = 0, Reservation_fails = 0
L2_cache_bank[11]: Access = 1336, Miss = 448, Miss_rate = 0.335, Pending_hits = 0, Reservation_fails = 0
L2_total_cache_accesses = 16188
L2_total_cache_misses = 5357
L2_total_cache_miss_rate = 0.3309
L2_total_cache_pending_hits = 30
L2_total_cache_reservation_fails = 343
L2_total_cache_breakdown:
	L2_cache_stats_breakdown[GLOBAL_ACC_R][HIT] = 10691
	L2_cache_stats_breakdown[GLOBAL_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][SECTOR_MISS] = 1
	L2_cache_stats_breakdown[LOCAL_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][HIT] = 22
	L2_cache_stats_breakdown[CONST_ACC_R][HIT_RESERVED] = 6
	L2_cache_stats_breakdown[CONST_ACC_R][MISS] = 1
	L2_cache_stats_breakdown[CONST_ACC_R][RESERVATION_FAIL] = 115
	L2_cache_stats_breakdown[CONST_ACC_R][SECTOR_MISS] = 1
	L2_cache_stats_breakdown[TEXTURE_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][HIT] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][MISS] = 1337
	L2_cache_stats_breakdown[GLOBAL_ACC_W][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][SECTOR_MISS] = 4009
	L2_cache_stats_breakdown[LOCAL_ACC_W][HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][HIT] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][MISS] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][HIT] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][MISS] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[INST_ACC_R][HIT] = 88
	L2_cache_stats_breakdown[INST_ACC_R][HIT_RESERVED] = 24
	L2_cache_stats_breakdown[INST_ACC_R][MISS] = 2
	L2_cache_stats_breakdown[INST_ACC_R][RESERVATION_FAIL] = 228
	L2_cache_stats_breakdown[INST_ACC_R][SECTOR_MISS] = 6
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][HIT] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][MISS] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][HIT] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][MISS] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][TOTAL_ACCESS] = 10692
	L2_cache_stats_breakdown[CONST_ACC_R][TOTAL_ACCESS] = 30
	L2_cache_stats_breakdown[GLOBAL_ACC_W][TOTAL_ACCESS] = 5346
	L2_cache_stats_breakdown[INST_ACC_R][TOTAL_ACCESS] = 120
L2_total_cache_reservation_fail_breakdown:
	L2_cache_stats_fail_breakdown[CONST_ACC_R][MSHR_MERGE_ENRTY_FAIL] = 115
	L2_cache_stats_fail_breakdown[INST_ACC_R][MSHR_MERGE_ENRTY_FAIL] = 228
L2_cache_data_port_util = 0.244
L2_cache_fill_port_util = 0.000

icnt_total_pkts_mem_to_simt=16188
icnt_total_pkts_simt_to_mem=8064
LD_mem_lat_dist  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
ST_mem_lat_dist  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
----------------------------Interconnect-DETAILS--------------------------------
Class 0:
Packet latency average = 86.4052
	minimum = 5
	maximum = 181
Network latency average = 78.4442
	minimum = 5
	maximum = 181
Slowest packet = 4920
Flit latency average = 68.6604
	minimum = 5
	maximum = 178
Slowest flit = 5076
Fragmentation average = 0.0427802
	minimum = 0
	maximum = 159
Injected packet rate average = 0.203292
	minimum = 0.0658894 (at node 1)
	maximum = 0.405098 (at node 15)
Accepted packet rate average = 0.203292
	minimum = 0.0902928 (at node 23)
	maximum = 0.315076 (at node 0)
Injected flit rate average = 0.243553
	minimum = 0.130965 (at node 1)
	maximum = 0.405098 (at node 15)
Accepted flit rate average= 0.243553
	minimum = 0.179772 (at node 23)
	maximum = 0.315076 (at node 0)
Injected packet length average = 1.19804
Accepted packet length average = 1.19804
Total in-flight flits = 0 (0 measured)
====== Overall Traffic Statistics ======
====== Traffic class 0 ======
Packet latency average = 86.4052 (1 samples)
	minimum = 5 (1 samples)
	maximum = 181 (1 samples)
Network latency average = 78.4442 (1 samples)
	minimum = 5 (1 samples)
	maximum = 181 (1 samples)
Flit latency average = 68.6604 (1 samples)
	minimum = 5 (1 samples)
	maximum = 178 (1 samples)
Fragmentation average = 0.0427802 (1 samples)
	minimum = 0 (1 samples)
	maximum = 159 (1 samples)
Injected packet rate average = 0.203292 (1 samples)
	minimum = 0.0658894 (1 samples)
	maximum = 0.405098 (1 samples)
Accepted packet rate average = 0.203292 (1 samples)
	minimum = 0.0902928 (1 samples)
	maximum = 0.315076 (1 samples)
Injected flit rate average = 0.243553 (1 samples)
	minimum = 0.130965 (1 samples)
	maximum = 0.405098 (1 samples)
Accepted flit rate average = 0.243553 (1 samples)
	minimum = 0.179772 (1 samples)
	maximum = 0.315076 (1 samples)
Injected packet size average = 1.19804 (1 samples)
Accepted packet size average = 1.19804 (1 samples)
Hops average = 1 (1 samples)
----------------------------END-of-Interconnect-DETAILS-------------------------


gpgpu_simulation_time = 0 days, 0 hrs, 0 min, 17 sec (17 sec)
gpgpu_simulation_rate = 70650 (inst/sec)
gpgpu_simulation_rate = 216 (cycle/sec)
gpgpu_silicon_slowdown = 3240740x
GPGPU-Sim: *** simulation thread starting and spinning waiting for work ***


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaThreadSynchronizeInternal(gpgpu_context*)" has been called.
GPGPU-Sim: synchronize waiting for inactive GPU simulation
GPGPU-Sim API: Stream Manager State
GPGPU-Sim: detected inactive GPU simulation thread


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaMemcpyInternal(void*, const void*, size_t, cudaMemcpyKind, gpgpu_context*)" has been called.
GPGPU-Sim PTX: cudaMemcpy(): devPtr = 0x7fdd3f06c800
GPGPU-Sim API: Stream Manager State
Jiayi Test: CUstream_st::print
GPGPU-Sim API:    stream 0 has 1 operations
GPGPU-Sim API:       0 :  stream operation memcpy device-to-host
GPGPU-Sim: ** START simulation thread (detected work) **
GPGPU-Sim API: Stream Manager State
Jiayi Test: CUstream_st::print
GPGPU-Sim API:    stream 0 has 1 operations
GPGPU-Sim API:       0 :  stream operation memcpy device-to-host
GPGPU-Sim API: stream 0 performing memcpy device-to-host
GPGPU-Sim PTX: copying 171056 bytes from GPU[0xc0053900] to CPU[0x7fdd3f06c800] ... done.
GPGPU-Sim: ** STOP simulation thread (no work) **
GPGPU-Sim: *** simulation thread starting and spinning waiting for work ***
1988 12 27  0 18 TONY       30.0  89.8  113   39 --> Distance=0.199997
1980 10 22 18  3 ISAAC      30.1  90.4  110  778 --> Distance=0.412312
1997 11 14 12 24 HELENE     30.5  89.8  134  529 --> Distance=0.538515
2003  8 27 12 10 TONY       29.9  89.4  160  286 --> Distance=0.608275
1974 12 22 18 24 JOYCE      30.6  89.9   80  593 --> Distance=0.608276


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaFree(void*)" has been called.


GPGPU-Sim PTX: CUDA API function "cudaError_t cudaFree(void*)" has been called.


GPGPU-Sim PTX: CUDA API function "void __cudaUnregisterFatBinary(void**)" has been called.
GPGPU-Sim: *** exit detected ***
